{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1e4f4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os\n",
    "import re\n",
    "import tempfile\n",
    "import random\n",
    "import itertools\n",
    "import pathlib\n",
    "import warnings\n",
    "import logging\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from scipy import sparse\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score, roc_curve, adjusted_rand_score\n",
    ")\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from tabpfn import TabPFNClassifier\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from pytorch_tabular import TabularModel\n",
    "from pytorch_tabular.config import DataConfig, TrainerConfig, OptimizerConfig\n",
    "from pytorch_tabular.models import TabTransformerConfig, FTTransformerConfig\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "# ─── Settings ────────────────────────────────────────────────\n",
    "DEVICE     = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ART_DIR    = pathlib.Path(\"artifacts\")\n",
    "DATA_DIR   = pathlib.Path(\"raw_data\")\n",
    "THR        = 0.5\n",
    "NEG_RATIO  = 1\n",
    "MODELS     = [\n",
    "    \"logreg\",\"tabnet\",\"tabpfn\",\"saint\",\n",
    "    \"tabtransformer\",\"fttransformer\",\"nars\",\n",
    "    \"ditto\",\"attendem\",\"hf_llm\"\n",
    "]\n",
    "N_RUNS     = 10\n",
    "NUM_WORKERS = multiprocessing.cpu_count()\n",
    "\n",
    "# ───────────────────────────── Baseline ──────────────────────────────\n",
    "class LogReg(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"Wrapper around sklearn.linear_model.LogisticRegression.\"\"\"\n",
    "    def __init__(self, penalty=\"l2\", C=1.0, solver=\"lbfgs\",\n",
    "                 max_iter=2000, n_jobs=-1, **kwargs):\n",
    "        self._lr = LogisticRegression(\n",
    "            penalty=penalty, C=C, solver=solver,\n",
    "            max_iter=max_iter, n_jobs=n_jobs, **kwargs\n",
    "        )\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        self._lr.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        return self._lr.predict(X)\n",
    "\n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        return self._lr.predict_proba(X)\n",
    "\n",
    "\n",
    "# ───────────────────── Tabular DL baselines ─────────────────────────\n",
    "class TabNet(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self):\n",
    "        self.tab = TabNetClassifier()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tab.fit(X=X, y=y, eval_set=[(X, y)],\n",
    "                     max_epochs=200, patience=20, verbose=0)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.tab.predict_proba(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.tab.predict(X)\n",
    "\n",
    "\n",
    "class TabPFN(TabPFNClassifier):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "            N_ensemble_configurations=32\n",
    "        )\n",
    "\n",
    "\n",
    "# ───────────────────── PyTorch-Tabular wrappers ─────────────────────\n",
    "class _PTab(BaseEstimator, ClassifierMixin):\n",
    "    CFG = None\n",
    "    def __init__(self, cat_cols: list[str], num_cols: list[str]):\n",
    "        self.cat, self.num = cat_cols, num_cols\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        df = pd.DataFrame(X, columns=self.num + self.cat)\n",
    "        df[\"_y_\"] = y\n",
    "        data_cfg = DataConfig(\n",
    "            target=[\"_y_\"],\n",
    "            continuous_cols=self.num,\n",
    "            categorical_cols=self.cat\n",
    "        )\n",
    "        trainer_cfg = TrainerConfig(\n",
    "            max_epochs=50, gpus=0, progress_bar=False\n",
    "        )\n",
    "        self.tm = TabularModel(\n",
    "            data_cfg, self.CFG(), trainer_cfg, OptimizerConfig()\n",
    "        )\n",
    "        self.tm.fit(train=df)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        df = pd.DataFrame(X, columns=self.num + self.cat)\n",
    "        p = self.tm.predict(df)[\"prediction\"].values\n",
    "        return np.vstack([1 - p, p]).T\n",
    "\n",
    "    def predict(self, X):\n",
    "        return (self.predict_proba(X)[:, 1] >= 0.5).astype(int)\n",
    "\n",
    "\n",
    "class TabTransformer(_PTab):\n",
    "    CFG = TabTransformerConfig\n",
    "\n",
    "\n",
    "class FTTransformer(_PTab):\n",
    "    CFG = FTTransformerConfig\n",
    "\n",
    "\n",
    "# ════════════════════  IN-FILE NARS IMPLEMENTATION  ════════════════════\n",
    "P_POOL_SIZE = 100\n",
    "N_PTRS      = 10\n",
    "\n",
    "ULTIMATE_COLS = [\n",
    "    \"ID\",\"Manntal\",\"Nafn\",\"Fornafn\",\"Millinafn\",\"Eftirnafn\",\"Aettarnafn\",\n",
    "    \"Faedingarar\",\"Kyn\",\"Stada\",\"Hjuskapur\",\"bi_einstaklingur\",\"bi_baer\",\n",
    "    \"bi_hreppur\",\"bi_sokn\",\"bi_sysla\",\"cleaned_status\",\"uniqueness_score\",\n",
    "    \"id_individual\",\"score\",\n",
    "]\n",
    "BAN_LIST = {\n",
    "    \"ID\",\"Millinafn\",\"Aettarnafn\",\"bi_hreppur\",\"bi_sokn\",\n",
    "    \"bi_sysla\",\"uniqueness_score\",\"id_individual\",\"score\"\n",
    "}\n",
    "\n",
    "\n",
    "class Truth:\n",
    "    def __init__(self, f: float, c: float):\n",
    "        self.f = f\n",
    "        self.c = max(min(c, 0.99), 0.01)\n",
    "\n",
    "    @property\n",
    "    def wp(self):\n",
    "        return self.f * self.c / (1 - self.c)\n",
    "\n",
    "    @property\n",
    "    def wn(self):\n",
    "        return (1 - self.f) * self.c / (1 - self.c)\n",
    "\n",
    "    def revise(self, other: \"Truth\"):\n",
    "        wp = self.wp + other.wp\n",
    "        wn = self.wn + other.wn\n",
    "        f  = wp / (wp + wn)\n",
    "        c  = (wp + wn) / (wp + wn + 1)\n",
    "        return Truth(f, c)\n",
    "\n",
    "    @property\n",
    "    def e(self):\n",
    "        return self.c * (self.f - 0.5) + 0.5\n",
    "\n",
    "\n",
    "class Pattern:\n",
    "    def __init__(self, stmts: set[str], truth: Truth):\n",
    "        self.statements = stmts\n",
    "        self.truth      = truth\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.statements)\n",
    "\n",
    "    def __hash__(self):\n",
    "        return sum(hash(s) for s in self.statements)\n",
    "\n",
    "    @property\n",
    "    def f(self):\n",
    "        return self.truth.f\n",
    "\n",
    "    @property\n",
    "    def c(self):\n",
    "        return self.truth.c\n",
    "\n",
    "    @property\n",
    "    def e(self):\n",
    "        return self.truth.e\n",
    "\n",
    "    def match(self, other: \"Pattern\"):\n",
    "        shared = self.statements & other.statements\n",
    "        a_un   = self.statements - shared\n",
    "        b_un   = other.statements - shared\n",
    "        matched= Pattern(shared, self.truth.revise(other.truth))\n",
    "        self_out = Pattern(a_un, self.truth)\n",
    "        other_out= Pattern(b_un, other.truth)\n",
    "        base_len = max(len(self), len(other)) or 1\n",
    "        sim = len(shared) / base_len\n",
    "        conf_e = max(self.truth.e, other.truth.e)\n",
    "        return (sim, conf_e), self_out, matched, other_out\n",
    "\n",
    "\n",
    "class PatternPool:\n",
    "    def __init__(self, size: int = P_POOL_SIZE):\n",
    "        self.size = size\n",
    "        self.pool: list[Pattern] = []\n",
    "\n",
    "    def add(self, p: Pattern):\n",
    "        # replace existing if same statements & higher confidence\n",
    "        for i, old in enumerate(self.pool):\n",
    "            if old.statements == p.statements:\n",
    "                if p.c > old.c:\n",
    "                    self.pool[i] = p\n",
    "                return\n",
    "        # insert sorted by e\n",
    "        for i, old in enumerate(self.pool):\n",
    "            if p.e > old.e:\n",
    "                self.pool.insert(i, p)\n",
    "                break\n",
    "        else:\n",
    "            self.pool.append(p)\n",
    "        # keep pool size in check\n",
    "        if len(self.pool) > self.size:\n",
    "            self.pool.pop(len(self.pool)//2)\n",
    "\n",
    "    def get_ptrs(self, n: int = N_PTRS):\n",
    "        half = n // 2\n",
    "        return set(self.pool[:half] + self.pool[-half:])\n",
    "\n",
    "\n",
    "def preprocess(r1: np.ndarray, r2: np.ndarray) -> Pattern:\n",
    "    stmts = set()\n",
    "    label = -1\n",
    "    for idx, (a, b) in enumerate(zip(r1, r2)):\n",
    "        col = ULTIMATE_COLS[idx] if idx < len(ULTIMATE_COLS) else f\"COL{idx}\"\n",
    "        if col in BAN_LIST:\n",
    "            continue\n",
    "        if col == \"Manntal\" and a and b:\n",
    "            diff = abs(float(a) - float(b))\n",
    "            stmts.add(f\"year_diff_{int(diff)}\")\n",
    "        elif col in (\"Nafn\",\"Fornafn\",\"Eftirnafn\",\"Faedingarar\",\"Kyn\",\"Hjuskapur\"):\n",
    "            eq = \"same\" if a == b else \"diff\"\n",
    "            stmts.add(f\"{eq}_{col.lower()}\")\n",
    "        elif col == \"bi_einstaklingur\":\n",
    "            label = 1 if a == b else 0\n",
    "    return Pattern(stmts, Truth(label if label != -1 else 0.5, 0.9))\n",
    "\n",
    "\n",
    "def match_ultimate(r1: np.ndarray, r2: np.ndarray,\n",
    "                   pool: PatternPool, n_ptr: int,\n",
    "                   just_eval: bool=False) -> float:\n",
    "    PTC = preprocess(r1, r2)\n",
    "    exps = []\n",
    "    for ptr in pool.get_ptrs(n_ptr):\n",
    "        (sim, e), a, b, c = PTC.match(ptr)\n",
    "        exps.append(Truth(e, sim))\n",
    "        if not just_eval:\n",
    "            if a.statements: pool.add(a)\n",
    "            if b.statements: pool.add(b)\n",
    "            if c.statements: pool.add(c)\n",
    "    if exps:\n",
    "        out = exps[0]\n",
    "        for t in exps[1:]:\n",
    "            out = out.revise(t)\n",
    "        return out.e\n",
    "    else:\n",
    "        if not just_eval:\n",
    "            pool.add(PTC)\n",
    "        return 0.5\n",
    "\n",
    "\n",
    "class NARS(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"Neuro-Analogical Reasoning System (symbolic).\"\"\"\n",
    "    def __init__(self, train_pairs: int=3000):\n",
    "        self.train_pairs = train_pairs\n",
    "        self.pool = PatternPool()\n",
    "\n",
    "    def _tok(self, s: str) -> np.ndarray:\n",
    "        return np.array([tok.strip() for tok in s.split(\" ; \")])\n",
    "\n",
    "    def fit(self, Xpair: np.ndarray, y=None):\n",
    "        for i in range(min(self.train_pairs, len(Xpair))):\n",
    "            a, b = Xpair[i]\n",
    "            match_ultimate(self._tok(a), self._tok(b),\n",
    "                           self.pool, N_PTRS, False)\n",
    "        return self\n",
    "\n",
    "    def _p(self, a: str, b: str) -> float:\n",
    "        return match_ultimate(self._tok(a), self._tok(b),\n",
    "                               self.pool, N_PTRS, True)\n",
    "\n",
    "    def predict_proba(self, Xpair: np.ndarray):\n",
    "        p = np.fromiter((self._p(a,b) for a,b in Xpair),\n",
    "                        float, len(Xpair))\n",
    "        return np.vstack([1-p, p]).T\n",
    "\n",
    "    def predict(self, Xpair: np.ndarray):\n",
    "        return (self.predict_proba(Xpair)[:,1] >= 0.5).astype(int)\n",
    "\n",
    "\n",
    "# ════════════════════  Minimal Ditto  ════════════════════════════\n",
    "class Ditto(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    Simplified Ditto: pair-wise transformer with CE + margin loss.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 plm: str=\"microsoft/deberta-v3-small\",\n",
    "                 max_len: int=256,\n",
    "                 lr: float=2e-5,\n",
    "                 margin: float=0.4,\n",
    "                 bs: int=16,\n",
    "                 epochs: int=3):\n",
    "        self.tok = AutoTokenizer.from_pretrained(plm)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            plm, num_labels=2).to(DEVICE)\n",
    "        self.bs, self.epochs = bs, epochs\n",
    "        self.lr, self.margin = lr, margin\n",
    "\n",
    "    @staticmethod\n",
    "    def _wrap(rec: str) -> str:\n",
    "        # assume rec is \"col1=val1 ; col2=val2 ; …\"\n",
    "        return \" \".join(f\"[COL] {kv}\" for kv in rec.split(\" ; \"))\n",
    "\n",
    "    def _encode(self, pairs):\n",
    "        texts = [\n",
    "            f\"{self._wrap(a)} [SEP] {self._wrap(b)}\"\n",
    "            for a, b in pairs\n",
    "        ]\n",
    "        return self.tok(\n",
    "            texts, truncation=True, padding=True,\n",
    "            max_length=256, return_tensors=\"pt\"\n",
    "        ).to(DEVICE)\n",
    "\n",
    "    def fit(self, Xpair: np.ndarray, y: np.ndarray):\n",
    "        enc = self._encode(Xpair)\n",
    "        labels = torch.tensor(y, device=DEVICE)\n",
    "        ds = TensorDataset(enc[\"input_ids\"], enc[\"attention_mask\"], labels)\n",
    "        loader = DataLoader(ds, batch_size=self.bs, shuffle=True)\n",
    "        opt = torch.optim.AdamW(self.model.parameters(), lr=self.lr)\n",
    "        ce  = nn.CrossEntropyLoss()\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            for ids, att, lab in loader:\n",
    "                opt.zero_grad()\n",
    "                logits = self.model(ids, attention_mask=att).logits\n",
    "                # margin loss with hardest negative\n",
    "                pos_scores = logits[lab==1][:,1]\n",
    "                neg_scores = logits[lab==0][:,1]\n",
    "                hard_neg = neg_scores.max() if len(neg_scores)>0 else 0.0\n",
    "                loss = ce(logits, lab) + torch.relu(self.margin - pos_scores + hard_neg).mean()\n",
    "                loss.backward(); opt.step()\n",
    "        return self\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict_proba(self, Xpair: np.ndarray):\n",
    "        enc = self._encode(Xpair)\n",
    "        logits = self.model(**enc).logits.softmax(-1)\n",
    "        p = logits[:,1].cpu().numpy()\n",
    "        return np.vstack([1-p, p]).T\n",
    "\n",
    "    def predict(self, Xpair: np.ndarray):\n",
    "        return (self.predict_proba(Xpair)[:,1] >= 0.5).astype(int)\n",
    "\n",
    "\n",
    "# ════════════════════  Minimal SAINT  ════════════════════════════\n",
    "class SAINTBlock(nn.Module):\n",
    "    \"\"\"One block: column-attn then row-attn.\"\"\"\n",
    "    def __init__(self, num_cols: int, d_model=64, nhead=4,\n",
    "                 d_ff=128, num_layers=3, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.col_emb = nn.Embedding(num_cols, d_model)\n",
    "        self.row_cls = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "        layers = []\n",
    "        for _ in range(num_layers):\n",
    "            layers.append(nn.TransformerEncoderLayer(\n",
    "                d_model, nhead, dim_feedforward=d_ff,\n",
    "                dropout=dropout, batch_first=True\n",
    "            ))  # column-wise\n",
    "            layers.append(nn.TransformerEncoderLayer(\n",
    "                d_model, nhead, dim_feedforward=d_ff,\n",
    "                dropout=dropout, batch_first=True\n",
    "            ))  # row-wise\n",
    "        self.tr = nn.Sequential(*layers)\n",
    "        self.out = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C)\n",
    "        B, C = x.shape\n",
    "        # embed each column\n",
    "        tok = x.unsqueeze(-1) + self.col_emb.weight[:C]\n",
    "        cls = self.row_cls.expand(B, -1, -1)\n",
    "        z = torch.cat([cls, tok], dim=1)      # (B, C+1, D)\n",
    "        h = self.tr(z)[:, 0]                  # take cls token\n",
    "        return self.out(h).squeeze(-1)        # (B,)\n",
    "\n",
    "\n",
    "class Saint(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    SAINT for tabular ER: alternating column+row attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_cols: int, epochs=80, lr=1e-3,\n",
    "                 d_model=64, nhead=4, d_ff=128, num_layers=3, dropout=0.1):\n",
    "        self.device = DEVICE\n",
    "        self.net = SAINTBlock(\n",
    "            num_cols, d_model, nhead, d_ff, num_layers, dropout\n",
    "        ).to(self.device)\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        X_t = torch.tensor(X, dtype=torch.float32, device=self.device)\n",
    "        y_t = torch.tensor(y.reshape(-1,1), dtype=torch.float32, device=self.device)\n",
    "        ds = TensorDataset(X_t, y_t)\n",
    "        loader = DataLoader(ds, batch_size=1024, shuffle=True)\n",
    "        opt = torch.optim.AdamW(self.net.parameters(), lr=self.lr)\n",
    "\n",
    "        self.net.train()\n",
    "        for _ in range(self.epochs):\n",
    "            for xb, yb in loader:\n",
    "                opt.zero_grad()\n",
    "                logits = self.net(xb)\n",
    "                loss = self.loss_fn(logits.unsqueeze(-1), yb)\n",
    "                loss.backward(); opt.step()\n",
    "        return self\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict_proba(self, X: np.ndarray):\n",
    "        X_t = torch.tensor(X, dtype=torch.float32, device=self.device)\n",
    "        logits = self.net(X_t).sigmoid().cpu().numpy()\n",
    "        return np.vstack([1-logits, logits]).T\n",
    "\n",
    "    def predict(self, X: np.ndarray):\n",
    "        return (self.predict_proba(X)[:,1] >= 0.5).astype(int)\n",
    "\n",
    "\n",
    "# ════════════════════  HFZeroShot ─═══════════════════════════\n",
    "class HFZeroShot(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    Zero-shot entity match via Llama-3 8B-Instruct GPTQ-4bit.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 repo_id=\"astronomer/Llama-3-8B-Instruct-GPTQ-4-Bit\",\n",
    "                 max_new=1):\n",
    "        cfg = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\"\n",
    "        )\n",
    "        self.tok = AutoTokenizer.from_pretrained(repo_id, use_fast=True)\n",
    "        self.llm = AutoModelForCausalLM.from_pretrained(\n",
    "            repo_id, device_map=\"auto\", quantization_config=cfg\n",
    "        ).eval()\n",
    "        self.max_new = max_new\n",
    "        self.yes = re.compile(r\"\\b(yes|sí|ja)\\b\", re.I)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict_proba(self, Xpair: np.ndarray):\n",
    "        prompts = [\n",
    "            f\"Same person? Yes or No.\\nA:{a}\\nB:{b}\\nAnswer:\"\n",
    "            for a, b in Xpair\n",
    "        ]\n",
    "        batch = self.tok(\n",
    "            prompts, return_tensors=\"pt\",\n",
    "            padding=True, truncation=True, max_length=512\n",
    "        ).to(self.llm.device)\n",
    "        gen = self.llm.generate(\n",
    "            **batch, max_new_tokens=self.max_new, temperature=0.0\n",
    "        )\n",
    "        outs = self.tok.batch_decode(\n",
    "            gen[:, batch[\"input_ids\"].shape[1]:],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        p = np.array([1.0 if self.yes.search(o) else 0.0 for o in outs])\n",
    "        return np.vstack([1-p, p]).T\n",
    "\n",
    "    def predict(self, Xpair: np.ndarray):\n",
    "        return (self.predict_proba(Xpair)[:,1] >= 0.5).astype(int)\n",
    "\n",
    "\n",
    "# ═════════════════════ registry helper ═════════════════════\n",
    "def _unavail(pkg):\n",
    "    raise ImportError(f\"Please pip install {pkg} to use this model.\")\n",
    "\n",
    "def get_model(\n",
    "    key: str,\n",
    "    cat_cols: list[str] | None = None,\n",
    "    num_cols: list[str] | None = None\n",
    "):\n",
    "    k = key.lower()\n",
    "    if k == \"logreg\":\n",
    "        return LogReg(max_iter=2000, n_jobs=-1)\n",
    "    if k == \"tabnet\":\n",
    "        return TabNet()\n",
    "    if k == \"tabpfn\":\n",
    "        return TabPFN()\n",
    "    if k == \"saint\":\n",
    "        if num_cols is None:\n",
    "            raise ValueError(\"SAINT requires `num_cols` length\")\n",
    "        return Saint(num_cols=len(num_cols))\n",
    "    if k == \"tabtransformer\":\n",
    "        return TabTransformer(cat_cols or [], num_cols or [])\n",
    "    if k == \"fttransformer\":\n",
    "        return FTTransformer(cat_cols or [], num_cols or [])\n",
    "    if k == \"nars\":\n",
    "        return NARS()\n",
    "    if k == \"ditto\":\n",
    "        return Ditto()\n",
    "    if k in (\"hf_llm\", \"llm\"):\n",
    "        return HFZeroShot()\n",
    "    raise ValueError(f\"Unknown model key: {key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4788bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:06:11 INFO === RUN_1 | WITHIN ===\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "a must be 1-dimensional",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 538\u001b[39m\n\u001b[32m    534\u001b[39m     avg.to_csv(\u001b[33m\"\u001b[39m\u001b[33maverage_results.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m538\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 520\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    517\u001b[39m idx_te = np.where(~np.isin(y, train_lbls))[\u001b[32m0\u001b[39m]\n\u001b[32m    519\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m scenario \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mwithin\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33macross\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m520\u001b[39m     df_run = \u001b[43mrun_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscenario\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    521\u001b[39m \u001b[43m                      \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mrun\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m                      \u001b[49m\u001b[43midx_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx_te\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m                      \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtxt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrng\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    524\u001b[39m     df_run[\u001b[33m\"\u001b[39m\u001b[33mscenario\u001b[39m\u001b[33m\"\u001b[39m] = scenario\n\u001b[32m    525\u001b[39m     all_runs.append(df_run.set_index(\u001b[33m\"\u001b[39m\u001b[33mscenario\u001b[39m\u001b[33m\"\u001b[39m, append=\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 445\u001b[39m, in \u001b[36mrun_task\u001b[39m\u001b[34m(scenario, tag, idx_tr, idx_te, X, y, txt, heim, rng)\u001b[39m\n\u001b[32m    442\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    443\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(scenario)\n\u001b[32m--> \u001b[39m\u001b[32m445\u001b[39m pairs_tr, y_tr = \u001b[43mmake_pairs_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrng\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    446\u001b[39m pairs_te, y_te = make_pairs_fn(idx_te, y, heim, rng)\n\u001b[32m    448\u001b[39m X_tr_sp = pair_matrix(X, pairs_tr)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 321\u001b[39m, in \u001b[36mmake_pairs_within\u001b[39m\u001b[34m(idxs, y, heim, rng)\u001b[39m\n\u001b[32m    319\u001b[39m pairs, labels = [], []\n\u001b[32m    320\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m sub \u001b[38;5;129;01min\u001b[39;00m buckets.values():\n\u001b[32m--> \u001b[39m\u001b[32m321\u001b[39m     p, l = \u001b[43mmake_pairs\u001b[49m\u001b[43m(\u001b[49m\u001b[43msub\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNEG_RATIO\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrng\u001b[49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# reuse old generator\u001b[39;00m\n\u001b[32m    322\u001b[39m     pairs.extend(p); labels = np.append(labels, l)\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m pairs[:MAX_PAIRS_PER_SPLIT], labels[:MAX_PAIRS_PER_SPLIT]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 398\u001b[39m, in \u001b[36mmake_pairs\u001b[39m\u001b[34m(idxs, y, neg_ratio, rng)\u001b[39m\n\u001b[32m    396\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    397\u001b[39m     want = \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inds) * (\u001b[38;5;28mlen\u001b[39m(inds) - \u001b[32m1\u001b[39m) // \u001b[32m2\u001b[39m, \u001b[32m200\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m     pos.extend(\u001b[43m_sample_pairs\u001b[49m\u001b[43m(\u001b[49m\u001b[43minds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrng\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    400\u001b[39m \u001b[38;5;66;03m# negatives\u001b[39;00m\n\u001b[32m    401\u001b[39m n_neg = \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mlen\u001b[39m(pos) * neg_ratio, MAX_PAIRS_PER_SPLIT - \u001b[38;5;28mlen\u001b[39m(pos))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 379\u001b[39m, in \u001b[36m_sample_pairs\u001b[39m\u001b[34m(bucket, k, rng)\u001b[39m\n\u001b[32m    377\u001b[39m k = \u001b[38;5;28mmin\u001b[39m(k, max_pairs)\n\u001b[32m    378\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m max_pairs <= \u001b[32m3_000\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m379\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrng\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mitertools\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcombinations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbucket\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplace\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m.tolist()\n\u001b[32m    380\u001b[39m pairs = \u001b[38;5;28mset\u001b[39m()\n\u001b[32m    381\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pairs) < k:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mnumpy/random/mtrand.pyx:947\u001b[39m, in \u001b[36mnumpy.random.mtrand.RandomState.choice\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: a must be 1-dimensional"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import itertools, logging, multiprocessing, pathlib, random, warnings, gc, os, re\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                             f1_score, roc_auc_score)\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "\n",
    "# ───────────────────────── constants ──────────────────────────\n",
    "DEVICE      = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_num_threads(4)                       # keep BLAS polite\n",
    "\n",
    "ART_DIR     = pathlib.Path(\"artifacts\")\n",
    "DATA_DIR    = pathlib.Path(\"raw_data\")\n",
    "\n",
    "THR                     = 0.5\n",
    "NEG_RATIO               = 1\n",
    "N_RUNS                  = 10\n",
    "NUM_WORKERS             = multiprocessing.cpu_count()\n",
    "MAX_PAIRS_PER_SPLIT     = 40_000        # hard cap per train / test split\n",
    "BATCH_SIZE_MATRIX       = 4_000         # rows per sparse-diff batch\n",
    "\n",
    "MODELS = [\n",
    "    \"logreg\", \"tabnet\", \"tabpfn\", \"saint\",\n",
    "    \"tabtransformer\", \"fttransformer\",\n",
    "    \"nars\", \"ditto\", \"attendem\", \"hf_llm\"\n",
    "]\n",
    "\n",
    "# ───────────────────── baseline logistic reg ──────────────────────\n",
    "class LogReg(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, penalty=\"l2\", C=1.0, solver=\"lbfgs\",\n",
    "                 max_iter=2000, n_jobs=-1, **kw):\n",
    "        self._lr = LogisticRegression(\n",
    "            penalty=penalty, C=C, solver=solver,\n",
    "            max_iter=max_iter, n_jobs=n_jobs, **kw)\n",
    "\n",
    "    def fit(self, X, y): self._lr.fit(X, y); return self\n",
    "    def predict(self, X): return self._lr.predict(X)\n",
    "    def predict_proba(self, X): return self._lr.predict_proba(X)\n",
    "\n",
    "# ─────────────────────── tabular DL baselines ─────────────────────\n",
    "class TabNet(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self): self.tab = TabNetClassifier()\n",
    "    def fit(self, X, y):\n",
    "        self.tab.fit(X=X, y=y, eval_set=[(X, y)],\n",
    "                     max_epochs=200, patience=20, verbose=0); return self\n",
    "    def predict_proba(self, X): return self.tab.predict_proba(X)\n",
    "    def predict(self, X): return self.tab.predict(X)\n",
    "\n",
    "class TabPFN(TabPFNClassifier):\n",
    "    def __init__(self):\n",
    "        super().__init__(device=(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "                         N_ensemble_configurations=32)\n",
    "\n",
    "# ────────────────────── PyTorch-Tabular wrappers ──────────────────\n",
    "class _PTab(BaseEstimator, ClassifierMixin):\n",
    "    CFG = None\n",
    "    def __init__(self, cat_cols:list[str], num_cols:list[str]):\n",
    "        self.cat, self.num = cat_cols, num_cols\n",
    "    def fit(self, X, y):\n",
    "        df = pd.DataFrame(X, columns=self.num+self.cat); df[\"_y_\"]=y\n",
    "        data_cfg = DataConfig(target=[\"_y_\"],\n",
    "                              continuous_cols=self.num,\n",
    "                              categorical_cols=self.cat)\n",
    "        trainer_cfg = TrainerConfig(max_epochs=50, gpus=0, progress_bar=False)\n",
    "        self.tm = TabularModel(data_cfg, self.CFG(), trainer_cfg, OptimizerConfig())\n",
    "        self.tm.fit(train=df); return self\n",
    "    def predict_proba(self, X):\n",
    "        df = pd.DataFrame(X, columns=self.num+self.cat)\n",
    "        p = self.tm.predict(df)[\"prediction\"].values\n",
    "        return np.vstack([1-p, p]).T\n",
    "    def predict(self, X): return (self.predict_proba(X)[:,1]>=0.5).astype(int)\n",
    "\n",
    "class TabTransformer(_PTab): CFG = TabTransformerConfig\n",
    "class FTTransformer(_PTab):  CFG = FTTransformerConfig\n",
    "\n",
    "# ───────────────────────── NARS (symbolic) ────────────────────────\n",
    "P_POOL_SIZE, N_PTRS = 100, 10\n",
    "ULTIMATE_COLS = [\"ID\",\"Manntal\",\"Nafn\",\"Fornafn\",\"Millinafn\",\"Eftirnafn\",\"Aettarnafn\",\n",
    "                 \"Faedingarar\",\"Kyn\",\"Stada\",\"Hjuskapur\",\"bi_einstaklingur\",\"bi_baer\",\n",
    "                 \"bi_hreppur\",\"bi_sokn\",\"bi_sysla\",\"cleaned_status\",\"uniqueness_score\",\n",
    "                 \"id_individual\",\"score\"]\n",
    "BAN_LIST = {\"ID\",\"Millinafn\",\"Aettarnafn\",\"bi_hreppur\",\"bi_sokn\",\n",
    "            \"bi_sysla\",\"uniqueness_score\",\"id_individual\",\"score\"}\n",
    "\n",
    "class Truth:\n",
    "    def __init__(self,f:float,c:float): self.f=f; self.c=max(min(c,0.99),0.01)\n",
    "    @property\n",
    "    def wp(self): return self.f*self.c/(1-self.c)\n",
    "    @property\n",
    "    def wn(self): return (1-self.f)*self.c/(1-self.c)\n",
    "    def revise(self,o:\"Truth\"):\n",
    "        wp,wn=self.wp+o.wp,self.wn+o.wn\n",
    "        return Truth(wp/(wp+wn),(wp+wn)/(wp+wn+1))\n",
    "    @property\n",
    "    def e(self): return self.c*(self.f-0.5)+0.5\n",
    "\n",
    "class Pattern:\n",
    "    def __init__(self,stmts:set[str],truth:Truth): self.statements, self.truth = stmts, truth\n",
    "    def __len__(self): return len(self.statements)\n",
    "    def __hash__(self): return sum(hash(s) for s in self.statements)\n",
    "    @property\n",
    "    def e(self): return self.truth.e\n",
    "    def match(self,o:\"Pattern\"):\n",
    "        sh=self.statements&o.statements\n",
    "        a,b=Pattern(self.statements-sh,self.truth),Pattern(o.statements-sh,o.truth)\n",
    "        m=Pattern(sh,self.truth.revise(o.truth)); base=max(len(self),len(o)) or 1\n",
    "        return (len(sh)/base,max(self.truth.e,o.truth.e)),a,m,b\n",
    "\n",
    "class PatternPool:\n",
    "    def __init__(self,size:int=P_POOL_SIZE): self.size=size; self.pool:list[Pattern]=[]\n",
    "    def add(self,p:Pattern):\n",
    "        for i,old in enumerate(self.pool):\n",
    "            if old.statements==p.statements and p.truth.c>old.truth.c:\n",
    "                self.pool[i]=p; return\n",
    "        for i,old in enumerate(self.pool):\n",
    "            if p.e>old.e: self.pool.insert(i,p); break\n",
    "        else: self.pool.append(p)\n",
    "        if len(self.pool)>self.size: self.pool.pop(len(self.pool)//2)\n",
    "    def get_ptrs(self,n:int=N_PTRS):\n",
    "        h=n//2; return set(self.pool[:h]+self.pool[-h:])\n",
    "\n",
    "def preprocess(r1:np.ndarray,r2:np.ndarray)->Pattern:\n",
    "    stmts,setlbl=set(),-1\n",
    "    for idx,(a,b) in enumerate(zip(r1,r2)):\n",
    "        col=ULTIMATE_COLS[idx] if idx<len(ULTIMATE_COLS) else f\"COL{idx}\"\n",
    "        if col in BAN_LIST: continue\n",
    "        if col==\"Manntal\" and a and b:\n",
    "            stmts.add(f\"year_diff_{int(abs(float(a)-float(b)))}\")\n",
    "        elif col in (\"Nafn\",\"Fornafn\",\"Eftirnafn\",\"Faedingarar\",\"Kyn\",\"Hjuskapur\"):\n",
    "            stmts.add((\"same_\" if a==b else \"diff_\")+col.lower())\n",
    "        elif col==\"bi_einstaklingur\": setlbl=1 if a==b else 0\n",
    "    return Pattern(stmts,Truth(setlbl if setlbl!=-1 else 0.5,0.9))\n",
    "\n",
    "def match_ultimate(r1,r2,pool:PatternPool,n_ptr:int,just_eval:bool=False)->float:\n",
    "    PTC=preprocess(r1,r2); exps=[]\n",
    "    for ptr in pool.get_ptrs(n_ptr):\n",
    "        (sim,e),a,m,b = PTC.match(ptr); exps.append(Truth(e,sim))\n",
    "        if not just_eval:\n",
    "            for p in (a,m,b):\n",
    "                if p.statements: pool.add(p)\n",
    "    if exps:\n",
    "        out=exps[0]\n",
    "        for t in exps[1:]: out=out.revise(t)\n",
    "        return out.e\n",
    "    if not just_eval: pool.add(PTC)\n",
    "    return 0.5\n",
    "\n",
    "class NARS(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self,train_pairs:int=3000):\n",
    "        self.train_pairs=train_pairs; self.pool=PatternPool()\n",
    "    def _tok(self,s:str): return np.array([t.strip() for t in s.split(\" ; \")])\n",
    "    def fit(self,Xpair,y=None):\n",
    "        for a,b in Xpair[:self.train_pairs]:\n",
    "            match_ultimate(self._tok(a),self._tok(b),self.pool,N_PTRS,False)\n",
    "        return self\n",
    "    def _p(self,a:str,b:str): return match_ultimate(self._tok(a),self._tok(b),self.pool,N_PTRS,True)\n",
    "    def predict_proba(self,Xpair):\n",
    "        p=np.fromiter((self._p(a,b) for a,b in Xpair),float,len(Xpair))\n",
    "        return np.vstack([1-p,p]).T\n",
    "    def predict(self,Xpair): return (self.predict_proba(Xpair)[:,1]>=0.5).astype(int)\n",
    "\n",
    "# ─────────────────────────── Ditto  (pair PLM) ────────────────────\n",
    "class Ditto(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self,plm=\"microsoft/deberta-v3-small\",max_len=256,lr=2e-5,\n",
    "                 margin=0.4,bs=16,epochs=3):\n",
    "        self.tok=AutoTokenizer.from_pretrained(plm)\n",
    "        self.model=AutoModelForSequenceClassification.from_pretrained(plm,num_labels=2).to(DEVICE)\n",
    "        self.bs,self.epochs,self.lr,self.margin = bs,epochs,lr,margin\n",
    "    @staticmethod\n",
    "    def _wrap(rec:str): return \" \".join(f\"[COL] {kv}\" for kv in rec.split(\" ; \"))\n",
    "    def _encode(self,pairs):\n",
    "        txt=[f\"{self._wrap(a)} [SEP] {self._wrap(b)}\" for a,b in pairs]\n",
    "        return self.tok(txt,truncation=True,padding=True,max_length=256,\n",
    "                        return_tensors=\"pt\").to(DEVICE)\n",
    "    def fit(self,Xpair,y):\n",
    "        enc=self._encode(Xpair)\n",
    "        labels=torch.tensor(y,device=DEVICE)\n",
    "        ds=TensorDataset(enc[\"input_ids\"],enc[\"attention_mask\"],labels)\n",
    "        loader=DataLoader(ds,batch_size=self.bs,shuffle=True,\n",
    "                          num_workers=NUM_WORKERS,pin_memory=True)\n",
    "        opt=torch.optim.AdamW(self.model.parameters(),lr=self.lr); ce=nn.CrossEntropyLoss()\n",
    "        for _ in range(self.epochs):\n",
    "            for ids,att,lab in loader:\n",
    "                opt.zero_grad()\n",
    "                logits=self.model(ids,attention_mask=att).logits\n",
    "                pos=logits[lab==1][:,1]; neg=logits[lab==0][:,1]\n",
    "                hard_neg=neg.max() if len(neg)>0 else 0.0\n",
    "                loss=ce(logits,lab)+torch.relu(self.margin-pos+hard_neg).mean()\n",
    "                loss.backward(); opt.step()\n",
    "        return self\n",
    "    @torch.no_grad()\n",
    "    def predict_proba(self,Xpair):\n",
    "        logits=self.model(**self._encode(Xpair)).logits.softmax(-1)[:,1].cpu().numpy()\n",
    "        return np.vstack([1-logits,logits]).T\n",
    "    def predict(self,Xpair): return (self.predict_proba(Xpair)[:,1]>=0.5).astype(int)\n",
    "\n",
    "# ───────────────────────────── SAINT ──────────────────────────────\n",
    "class SAINTBlock(nn.Module):\n",
    "    def __init__(self,num_cols:int,d_model=64,nhead=4,d_ff=128,\n",
    "                 num_layers=3,dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.col_emb=nn.Embedding(num_cols,d_model)\n",
    "        self.row_cls=nn.Parameter(torch.randn(1,1,d_model))\n",
    "        layers=[]\n",
    "        for _ in range(num_layers):\n",
    "            layers.append(nn.TransformerEncoderLayer(d_model,nhead,d_ff,dropout,batch_first=True))\n",
    "            layers.append(nn.TransformerEncoderLayer(d_model,nhead,d_ff,dropout,batch_first=True))\n",
    "        self.tr=nn.Sequential(*layers); self.out=nn.Linear(d_model,1)\n",
    "    def forward(self,x):\n",
    "        B,C=x.shape\n",
    "        tok=x.unsqueeze(-1)+self.col_emb.weight[:C]\n",
    "        cls=self.row_cls.expand(B,-1,-1)\n",
    "        h=self.tr(torch.cat([cls,tok],1))[:,0]\n",
    "        return self.out(h).squeeze(-1)\n",
    "\n",
    "class Saint(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self,num_cols:int,epochs=80,lr=1e-3,d_model=64,\n",
    "                 nhead=4,d_ff=128,num_layers=3,dropout=0.1):\n",
    "        self.net=SAINTBlock(num_cols,d_model,nhead,d_ff,num_layers,dropout).to(DEVICE)\n",
    "        self.epochs, self.lr = epochs,lr; self.loss=nn.BCEWithLogitsLoss()\n",
    "    def fit(self,X,y):\n",
    "        X_t=torch.tensor(X,dtype=torch.float32,device=DEVICE)\n",
    "        y_t=torch.tensor(y.reshape(-1,1),dtype=torch.float32,device=DEVICE)\n",
    "        ds=TensorDataset(X_t,y_t)\n",
    "        loader=DataLoader(ds,batch_size=1024,shuffle=True,\n",
    "                          num_workers=NUM_WORKERS,pin_memory=True)\n",
    "        opt=torch.optim.AdamW(self.net.parameters(),lr=self.lr)\n",
    "        self.net.train()\n",
    "        for _ in range(self.epochs):\n",
    "            for xb,yb in loader:\n",
    "                opt.zero_grad()\n",
    "                loss=self.loss(self.net(xb).unsqueeze(-1), yb)\n",
    "                loss.backward(); opt.step()\n",
    "        return self\n",
    "    @torch.no_grad()\n",
    "    def predict_proba(self,X):\n",
    "        p=self.net(torch.tensor(X,dtype=torch.float32,device=DEVICE)).sigmoid().cpu().numpy()\n",
    "        return np.vstack([1-p,p]).T\n",
    "    def predict(self,X): return (self.predict_proba(X)[:,1]>=0.5).astype(int)\n",
    "\n",
    "# ───────────────────── HF Llama-3 zero-shot baseline ───────────────\n",
    "class HFZeroShot(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self,repo_id=\"astronomer/Llama-3-8B-Instruct-GPTQ-4-Bit\",max_new=1):\n",
    "        cfg=BitsAndBytesConfig(load_in_4bit=True,bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                               bnb_4bit_use_double_quant=True,bnb_4bit_quant_type=\"nf4\")\n",
    "        self.tok=AutoTokenizer.from_pretrained(repo_id,use_fast=True)\n",
    "        self.llm=AutoModelForCausalLM.from_pretrained(repo_id,device_map=\"auto\",\n",
    "                                                     quantization_config=cfg).eval()\n",
    "        self.max_new=max_new; self.yes=re.compile(r\"\\b(yes|sí|ja)\\b\",re.I)\n",
    "    def fit(self,X,y=None): return self\n",
    "    @torch.no_grad()\n",
    "    def predict_proba(self,Xpair):\n",
    "        prompts=[f\"Same person? Yes or No.\\nA:{a}\\nB:{b}\\nAnswer:\" for a,b in Xpair]\n",
    "        batch=self.tok(prompts,return_tensors=\"pt\",padding=True,truncation=True,\n",
    "                       max_length=512).to(self.llm.device)\n",
    "        gen=self.llm.generate(**batch,max_new_tokens=self.max_new,temperature=0.0)\n",
    "        outs=self.tok.batch_decode(gen[:,batch[\"input_ids\"].shape[1]:],\n",
    "                                   skip_special_tokens=True)\n",
    "        p=np.array([1.0 if self.yes.search(o) else 0.0 for o in outs])\n",
    "        return np.vstack([1-p,p]).T\n",
    "    def predict(self,Xpair): return (self.predict_proba(Xpair)[:,1]>=0.5).astype(int)\n",
    "\n",
    "# ─────────────────────────── registry helper ──────────────────────\n",
    "def get_model(key:str,cat_cols:list[str]|None=None,num_cols:list[str]|None=None):\n",
    "    k=key.lower()\n",
    "    if k==\"logreg\": return LogReg(max_iter=2000,n_jobs=-1)\n",
    "    if k==\"tabnet\": return TabNet()\n",
    "    if k==\"tabpfn\": return TabPFN()\n",
    "    if k==\"saint\":\n",
    "        if num_cols is None: raise ValueError(\"SAINT requires `num_cols`\")\n",
    "        return Saint(num_cols=len(num_cols))\n",
    "    if k==\"tabtransformer\": return TabTransformer(cat_cols or [],num_cols or [])\n",
    "    if k==\"fttransformer\":  return FTTransformer(cat_cols or [],num_cols or [])\n",
    "    if k==\"nars\": return NARS()\n",
    "    if k==\"ditto\": return Ditto()\n",
    "    if k in (\"hf_llm\",\"llm\"): return HFZeroShot()\n",
    "    raise ValueError(f\"Unknown model key: {key}\")\n",
    "\n",
    "# ───────────────────────── helpers ───────────────────────────────\n",
    "def load_people() -> pd.DataFrame:\n",
    "    ppl = (pd.read_csv(DATA_DIR / \"people.csv\", low_memory=False)\n",
    "             .rename(columns=str.lower)\n",
    "             .set_index(\"id\"))\n",
    "    for c in [\"first_name\", \"middle_name\", \"patronym\", \"surname\"]:\n",
    "        if c not in ppl.columns:\n",
    "            ppl[c] = \"\"\n",
    "    ppl[\"full_name\"] = (ppl[[\"first_name\", \"middle_name\",\n",
    "                             \"patronym\", \"surname\"]]\n",
    "                        .fillna(\"\")\n",
    "                        .apply(lambda r: \" \".join(w.strip().lower()\n",
    "                                                  for w in r if w), axis=1))\n",
    "    lbl = (pd.read_csv(ART_DIR / \"row_labels.csv\")\n",
    "             .set_index(\"row_id\")[\"person\"]\n",
    "             .pipe(pd.to_numeric, errors=\"coerce\"))\n",
    "    ppl[\"person\"] = ppl.index.to_series().map(lbl)\n",
    "    return ppl.reset_index()\n",
    "\n",
    "\n",
    "# ─────────── pair-sampling primitives with hard caps ───────────\n",
    "def _sample_pairs(bucket: List[int], k: int,\n",
    "                  rng: np.random.RandomState) -> List[Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Uniformly sample up to k unordered pairs from `bucket`\n",
    "    without ever constructing a 2-D object array (fixes ValueError).\n",
    "    \"\"\"\n",
    "    n = len(bucket)\n",
    "    if n < 2 or k == 0:\n",
    "        return []\n",
    "    all_pairs = list(itertools.combinations(bucket, 2))\n",
    "    if len(all_pairs) <= k:\n",
    "        return all_pairs\n",
    "    idx = rng.choice(len(all_pairs), k, replace=False)\n",
    "    return [all_pairs[i] for i in idx]\n",
    "\n",
    "\n",
    "def make_pairs(idxs: np.ndarray, y: np.ndarray, neg_ratio: int,\n",
    "               rng: np.random.RandomState):\n",
    "    \"\"\"Generic positive / negative generator with per-label cap.\"\"\"\n",
    "    lab2idx: dict[int, list[int]] = {}\n",
    "    for i in idxs:\n",
    "        lab2idx.setdefault(y[i], []).append(i)\n",
    "\n",
    "    # positives – capped per label\n",
    "    pos = []\n",
    "    for inds in lab2idx.values():\n",
    "        if len(inds) < 2:\n",
    "            continue\n",
    "        want = min(len(inds) * (len(inds) - 1) // 2, 200)\n",
    "        pos.extend(_sample_pairs(inds, want, rng))\n",
    "\n",
    "    # negatives\n",
    "    n_neg = min(len(pos) * neg_ratio, MAX_PAIRS_PER_SPLIT - len(pos))\n",
    "    neg = set()\n",
    "    labels = list(lab2idx)\n",
    "    while len(neg) < n_neg and len(labels) > 1:\n",
    "        l1, l2 = rng.choice(labels, 2, replace=False)\n",
    "        neg.add(tuple(sorted((rng.choice(lab2idx[l1]),\n",
    "                              rng.choice(lab2idx[l2])))))\n",
    "\n",
    "    pairs  = pos + list(neg)\n",
    "    if len(pairs) > MAX_PAIRS_PER_SPLIT:\n",
    "        pairs = rng.choice(pairs, MAX_PAIRS_PER_SPLIT, replace=False).tolist()\n",
    "\n",
    "    labels_arr = np.array([1] * len(pos) + [0] * len(neg), dtype=np.int8)\n",
    "    return pairs, labels_arr\n",
    "\n",
    "\n",
    "def make_pairs_within(idxs: np.ndarray, y: np.ndarray,\n",
    "                      heim: np.ndarray, rng: np.random.RandomState):\n",
    "    \"\"\"Pairs only among rows that share the SAME heimild value.\"\"\"\n",
    "    buckets: dict[int, list[int]] = {}\n",
    "    for i in idxs:\n",
    "        buckets.setdefault(heim[i], []).append(i)\n",
    "\n",
    "    pairs, label_chunks = [], []\n",
    "    for sub in buckets.values():\n",
    "        p, l = make_pairs(sub, y, NEG_RATIO, rng)\n",
    "        pairs.extend(p); label_chunks.append(l)\n",
    "\n",
    "    labels = np.concatenate(label_chunks) if label_chunks else np.empty(0, dtype=np.int8)\n",
    "    return pairs[:MAX_PAIRS_PER_SPLIT], labels[:MAX_PAIRS_PER_SPLIT]\n",
    "\n",
    "\n",
    "def make_pairs_across(idxs: np.ndarray, y: np.ndarray,\n",
    "                      heim: np.ndarray, rng: np.random.RandomState):\n",
    "    \"\"\"Pairs whose two rows come from DIFFERENT heimild values.\"\"\"\n",
    "    lab2idx: dict[int, list[int]] = {}\n",
    "    for i in idxs:\n",
    "        lab2idx.setdefault(y[i], []).append(i)\n",
    "\n",
    "    pos = []\n",
    "    for inds in lab2idx.values():\n",
    "        if len(inds) < 2:\n",
    "            continue\n",
    "        by_census: dict[int, list[int]] = {}\n",
    "        for i in inds:\n",
    "            by_census.setdefault(heim[i], []).append(i)\n",
    "        if len(by_census) < 2:\n",
    "            continue\n",
    "        c_keys = list(by_census)\n",
    "        for a in range(len(c_keys)):\n",
    "            for b in range(a + 1, len(c_keys)):\n",
    "                pairs_ab = list(itertools.product(by_census[c_keys[a]],\n",
    "                                                  by_census[c_keys[b]]))\n",
    "                if len(pairs_ab) > 200:\n",
    "                    pairs_ab = rng.choice(pairs_ab, 200, replace=False).tolist()\n",
    "                pos.extend(pairs_ab)\n",
    "\n",
    "    # negatives = diff person & diff census\n",
    "    neg = set()\n",
    "    want_neg = min(len(pos) * NEG_RATIO,\n",
    "                   MAX_PAIRS_PER_SPLIT - len(pos))\n",
    "    while len(neg) < want_neg:\n",
    "        i, j = rng.choice(idxs, 2, replace=False)\n",
    "        if y[i] != y[j] and heim[i] != heim[j]:\n",
    "            neg.add(tuple(sorted((i, j))))\n",
    "\n",
    "    pairs = pos + list(neg)\n",
    "    if len(pairs) > MAX_PAIRS_PER_SPLIT:\n",
    "        pairs = rng.choice(pairs, MAX_PAIRS_PER_SPLIT, replace=False).tolist()\n",
    "\n",
    "    labels = np.array([1] * len(pos) + [0] * len(neg), dtype=np.int8)\n",
    "    return pairs, labels\n",
    "\n",
    "\n",
    "def pair_matrix(X: sparse.spmatrix, pairs: List[Tuple[int, int]],\n",
    "                batch_size: int = BATCH_SIZE_MATRIX) -> sparse.csr_matrix:\n",
    "    \"\"\"Build |pairs|×d absolute-difference matrix in memory-capped batches.\"\"\"\n",
    "    rows = []\n",
    "    for i in range(0, len(pairs), batch_size):\n",
    "        chunk = pairs[i:i+batch_size]\n",
    "        a = X[[p[0] for p in chunk]]\n",
    "        b = X[[p[1] for p in chunk]]\n",
    "        rows.append(abs(a - b))\n",
    "    return sparse.vstack(rows).tocsr()\n",
    "\n",
    "# ───────────────────────── run one task ──────────────────────────\n",
    "_DENSE_MODELS = {\"tabnet\", \"tabpfn\", \"saint\",\n",
    "                 \"tabtransformer\", \"fttransformer\", \"logreg\"}\n",
    "\n",
    "def run_task(scenario: str, tag: str,\n",
    "             idx_tr, idx_te, X, y, txt, heim,\n",
    "             rng: np.random.RandomState):\n",
    "    logging.info(\"=== %s | %s ===\", tag.upper(), scenario.upper())\n",
    "\n",
    "    make_pairs_fn = make_pairs_within if scenario == \"within\" else make_pairs_across\n",
    "    pairs_tr, y_tr = make_pairs_fn(idx_tr, y, heim, rng)\n",
    "    pairs_te, y_te = make_pairs_fn(idx_te, y, heim, rng)\n",
    "\n",
    "    X_tr_sp = pair_matrix(X, pairs_tr)\n",
    "    X_te_sp = pair_matrix(X, pairs_te)\n",
    "\n",
    "    X_tr_dense = X_tr_sp.A.astype(np.float32)\n",
    "    X_te_dense = X_te_sp.A.astype(np.float32)\n",
    "    num_cols   = [f\"f{i}\" for i in range(X_tr_dense.shape[1])]\n",
    "\n",
    "    results = []\n",
    "    for name in MODELS:\n",
    "        try:\n",
    "            mdl = get_model(name, [], num_cols)\n",
    "\n",
    "            if name in {\"nars\", \"ditto\", \"attendem\", \"hf_llm\"}:\n",
    "                tr_txt = np.column_stack([txt[[i for i, _ in pairs_tr]],\n",
    "                                          txt[[j for _, j in pairs_tr]]])\n",
    "                te_txt = np.column_stack([txt[[i for i, _ in pairs_te]],\n",
    "                                          txt[[j for _, j in pairs_te]]])\n",
    "                mdl.fit(tr_txt, y_tr)\n",
    "                probs = mdl.predict_proba(te_txt)[:, 1]\n",
    "            else:\n",
    "                Xtr = X_tr_dense if name in _DENSE_MODELS else X_tr_sp\n",
    "                Xte = X_te_dense if name in _DENSE_MODELS else X_te_sp\n",
    "                mdl.fit(Xtr, y_tr)\n",
    "                probs = mdl.predict_proba(Xte)[:, 1]\n",
    "\n",
    "            pred = (probs >= THR).astype(np.int8)\n",
    "            results.append({\n",
    "                \"model\":     name,\n",
    "                \"accuracy\":  accuracy_score(y_te, pred),\n",
    "                \"precision\": precision_score(y_te, pred, zero_division=0),\n",
    "                \"recall\":    recall_score(y_te, pred, zero_division=0),\n",
    "                \"f1\":        f1_score(y_te, pred, zero_division=0),\n",
    "                \"roc_auc\":   roc_auc_score(y_te, probs),\n",
    "            })\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"{name} failed: {e}\")\n",
    "\n",
    "    return pd.DataFrame(results).set_index(\"model\")\n",
    "\n",
    "\n",
    "# ─────────────────────────── main loop ───────────────────────────\n",
    "def main():\n",
    "    logging.basicConfig(level=logging.INFO,\n",
    "                        format=\"%(asctime)s %(levelname)s %(message)s\",\n",
    "                        datefmt=\"%H:%M:%S\")\n",
    "\n",
    "    ppl  = load_people()\n",
    "    X    = sparse.load_npz(ART_DIR / \"iceid_ml_ready.npz\")\n",
    "    y    = ppl[\"person\"].values\n",
    "    txt  = ppl[\"full_name\"].values\n",
    "    heim = ppl[\"heimild\"].values\n",
    "\n",
    "    # drop NaNs & singletons\n",
    "    base = (~np.isnan(y)) & (y != -1)\n",
    "    freq = pd.Series(y[base]).value_counts()\n",
    "    good = freq[freq >= 2].index\n",
    "    keep = base & np.isin(y, good)\n",
    "\n",
    "    X, y, txt, heim = X[keep], y[keep], txt[keep], heim[keep]\n",
    "    labels = np.unique(y)\n",
    "\n",
    "    all_runs = []\n",
    "    for run in range(N_RUNS):\n",
    "        rng = np.random.RandomState(run)\n",
    "\n",
    "        train_lbls = rng.choice(labels,\n",
    "                                size=int(0.8 * len(labels)),\n",
    "                                replace=False)\n",
    "        idx_tr = np.where(np.isin(y, train_lbls))[0]\n",
    "        idx_te = np.where(~np.isin(y, train_lbls))[0]\n",
    "\n",
    "        for scenario in (\"within\", \"across\"):\n",
    "            df_run = run_task(scenario,\n",
    "                              f\"run_{run+1}\",\n",
    "                              idx_tr, idx_te,\n",
    "                              X, y, txt, heim, rng)\n",
    "            df_run[\"scenario\"] = scenario\n",
    "            all_runs.append(df_run.set_index(\"scenario\", append=True))\n",
    "\n",
    "        torch.cuda.empty_cache(); gc.collect()\n",
    "\n",
    "    avg = (pd.concat(all_runs)\n",
    "             .swaplevel()          # index = (scenario, model)\n",
    "             .groupby(level=[0,1]).mean())\n",
    "\n",
    "    print(\"\\n=== AVERAGE OVER RUNS ===\")\n",
    "    print(avg.round(4))\n",
    "    avg.to_csv(\"average_results.csv\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ICE-ID-2.0-K4yMlCOc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
