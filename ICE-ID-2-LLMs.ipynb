{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9c49497",
   "metadata": {},
   "source": [
    "# MODELS & ALGOs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4035d692",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12:01:10 INFO ‚ñ∂ load people.csv ‚Ä¶\n",
      "12:01:11 INFO ‚è± load people.csv: 1.9s\n",
      "12:01:15 INFO ‚ñ∂ build trigram blocks ‚Ä¶\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10e2c084f9364d4eb448b4ed8bc629e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "index blocks:   0%|          | 0/476683 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12:01:27 INFO ‚è± build trigram blocks: 12.7s\n",
      "12:01:27 INFO blocks: 161413\n",
      "12:01:28 INFO === RUN 1/10 ===\n",
      "12:01:28 INFO ‚ñ∂ pair sampling within ‚Ä¶\n",
      "12:28:05 INFO ‚è± pair sampling within: 1597.6s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 240\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m MODES:\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Timer(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mpair sampling \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m         p_tr, p_te = \u001b[43msample_pairs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_lab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtr_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mte_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[43m                                  \u001b[49m\u001b[43mMAX_TR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMAX_TE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrng\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    242\u001b[39m     y_tr, y_te = labels_for(p_tr, df_lab), labels_for(p_te, df_lab)\n\u001b[32m    244\u001b[39m     tb = TriBERTaER();   tb_dir = OUTPUT/\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_triberta_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 105\u001b[39m, in \u001b[36msample_pairs\u001b[39m\u001b[34m(blocks, df, tr_idx, te_idx, k_tr, k_te, mode, rng)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m bucket \u001b[38;5;129;01min\u001b[39;00m rng.sample(\u001b[38;5;28mlist\u001b[39m(blocks.values()), \u001b[38;5;28mlen\u001b[39m(blocks)):\n\u001b[32m    104\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tr) < k_tr:\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m         tr += [p \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_pairs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbucket\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtr_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43mk_tr\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrng\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m keep(*p)]\n\u001b[32m    107\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(te) < k_te:\n\u001b[32m    108\u001b[39m         te += [p \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m _pairs([i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m bucket \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m te_idx],\n\u001b[32m    109\u001b[39m                                  k_te - \u001b[38;5;28mlen\u001b[39m(te), rng) \u001b[38;5;28;01mif\u001b[39;00m keep(*p)]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 95\u001b[39m, in \u001b[36m_pairs\u001b[39m\u001b[34m(bucket, k, rng)\u001b[39m\n\u001b[32m     93\u001b[39m seen = \u001b[38;5;28mset\u001b[39m()\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(seen) < k:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m     seen.add(\u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28msorted\u001b[39m(rng.sample(bucket, \u001b[32m2\u001b[39m))))\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(seen)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# ICE‚ÄëID ER Runner ¬∑ TriBERTa‚ÄëER | DistilBERT | MiniLM‚ÄëCE\n",
    "# dual task (within / across census) ¬∑ thread pool ¬∑ resumable ¬∑ verbose progress\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "import os, itertools, json, logging, gc, random, time\n",
    "from pathlib import Path\n",
    "from multiprocessing import cpu_count\n",
    "from multiprocessing.dummy import Pool                     # thread‚Äëbased\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n",
    "                          Trainer, TrainingArguments)\n",
    "from sentence_transformers import (SentenceTransformer, CrossEncoder,\n",
    "                                   InputExample, losses)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ config ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "ART_DIR   = Path(\"artifacts\")\n",
    "DATA_DIR  = Path(\"raw_data\")\n",
    "OUTPUT    = Path(\"models_er\"); OUTPUT.mkdir(exist_ok=True)\n",
    "DEVICE    = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "N_RUNS          = 10\n",
    "MAX_TR, MAX_TE  = 100_000, 100_000\n",
    "PAIR_SAMPLES    = 25_000\n",
    "BATCH_EMB       = 512\n",
    "DISTIL_BS       = 16\n",
    "MINILM_BS       = 16\n",
    "RNG_SEED        = 42\n",
    "CKPT_FILE       = OUTPUT / \"checkpoint.json\"\n",
    "MODES           = (\"within\", \"across\")\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(levelname)s %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\"\n",
    ")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "class Timer:\n",
    "    def __init__(self, msg):\n",
    "        self.msg, self.t0 = msg, time.perf_counter()\n",
    "        logging.info(\"‚ñ∂ %s ‚Ä¶\", self.msg)\n",
    "    def __enter__(self): return self\n",
    "    def __exit__(self, *_):\n",
    "        logging.info(\"‚è± %s: %.1fs\", self.msg, time.perf_counter() - self.t0)\n",
    "\n",
    "def load_people():\n",
    "    with Timer(\"load people.csv\"):\n",
    "        ppl = (pd.read_csv(DATA_DIR/\"people.csv\", low_memory=False)\n",
    "                 .rename(columns=str.lower).set_index(\"id\"))\n",
    "    for c in (\"first_name\",\"middle_name\",\"patronym\",\"surname\"):\n",
    "        ppl[c] = ppl.get(c,\"\").fillna(\"\").astype(str)\n",
    "    ppl[\"full_name\"] = ppl[[\"first_name\",\"middle_name\",\"patronym\",\"surname\"]] \\\n",
    "        .apply(lambda r: \" \".join(w.strip().lower() for w in r if w), axis=1)\n",
    "    for c in (\"birthyear\",\"heimild\"):\n",
    "        ppl[c] = pd.to_numeric(ppl.get(c,0), errors=\"coerce\").fillna(0).astype(int)\n",
    "    lbl = (pd.read_csv(ART_DIR/\"row_labels.csv\")\n",
    "             .set_index(\"row_id\")[\"person\"]\n",
    "             .pipe(pd.to_numeric, errors=\"coerce\").astype(\"Int64\"))\n",
    "    ppl[\"person\"] = pd.Series(ppl.index, index=ppl.index).map(lbl)\n",
    "    return ppl.reset_index()\n",
    "\n",
    "def trigram(s):\n",
    "    s = \"\".join(c for c in s.lower() if c.isalnum())\n",
    "    return {s[i:i+3] for i in range(len(s)-2)} if len(s) >= 3 else {s}\n",
    "\n",
    "def build_blocks(df):\n",
    "    with Timer(\"build trigram blocks\"):\n",
    "        with Pool(cpu_count()) as p:\n",
    "            df = df.copy(); df[\"trigs\"] = list(p.map(trigram, df[\"full_name\"].tolist()))\n",
    "        blocks = {}\n",
    "        for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"index blocks\"):\n",
    "            key = (frozenset(row[\"trigs\"]),\n",
    "                   (row[\"birthyear\"]//10) if row[\"birthyear\"] > 0 else -1)\n",
    "            blocks.setdefault(key, []).append(idx)\n",
    "    logging.info(\"blocks: %d\", len(blocks))\n",
    "    return blocks\n",
    "\n",
    "def _pairs(bucket, k, rng):\n",
    "    n = len(bucket); max_p = n*(n-1)//2\n",
    "    if n < 2 or k == 0: return []\n",
    "    if max_p <= 3_000:\n",
    "        return rng.sample(list(itertools.combinations(bucket, 2)), min(k, max_p))\n",
    "    seen = set()\n",
    "    while len(seen) < k:\n",
    "        seen.add(tuple(sorted(rng.sample(bucket, 2))))\n",
    "    return list(seen)\n",
    "\n",
    "def sample_pairs(blocks, df, tr_idx, te_idx, k_tr, k_te, mode, rng):\n",
    "    def keep(i, j):\n",
    "        same = df.at[i, \"heimild\"] == df.at[j, \"heimild\"] and df.at[i, \"heimild\"] > 0\n",
    "        return same if mode == \"within\" else not same\n",
    "\n",
    "    tr, te = [], []\n",
    "    for bucket in tqdm(rng.sample(list(blocks.values()), len(blocks)),\n",
    "                       total=len(blocks),\n",
    "                       desc=f\"sampling {mode}\",\n",
    "                       leave=False):\n",
    "        if len(tr) < k_tr:\n",
    "            tr += [p for p in _pairs([i for i in bucket if i in tr_idx],\n",
    "                                      k_tr - len(tr), rng) if keep(*p)]\n",
    "        if len(te) < k_te:\n",
    "            te += [p for p in _pairs([i for i in bucket if i in te_idx],\n",
    "                                      k_te - len(te), rng) if keep(*p)]\n",
    "        if len(tr) >= k_tr and len(te) >= k_te:\n",
    "            break\n",
    "    return tr[:k_tr], te[:k_te]\n",
    "\n",
    "def labels_for(pairs, df):\n",
    "    return np.array([int(df.at[i,\"person\"] == df.at[j,\"person\"]\n",
    "                         and not pd.isna(df.at[i,\"person\"])) for i,j in pairs], int)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ models ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "class TriBERTaER:\n",
    "    def __init__(self, name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        self.model = SentenceTransformer(name, device=DEVICE)\n",
    "        self.loss  = losses.CosineSimilarityLoss(self.model)\n",
    "    @staticmethod\n",
    "    def serial(row): return \" ; \".join(f\"{k}: {v}\" for k,v in row.items())\n",
    "    def train(self, df, idx, out_dir, n_pairs=PAIR_SAMPLES):\n",
    "        rng, examples = random.Random(RNG_SEED), []\n",
    "        grouped = df.loc[idx].dropna(subset=[\"person\"]).groupby(\"person\")\n",
    "        persons = list(grouped.groups)\n",
    "        while len(examples) < n_pairs:\n",
    "            pid = rng.choice(persons)\n",
    "            rows = grouped.get_group(pid).index.tolist()\n",
    "            if len(rows) >= 2:\n",
    "                a, p = rng.sample(rows, 2)\n",
    "                examples.append(InputExample(\n",
    "                    texts=[self.serial(df.loc[a]), self.serial(df.loc[p])], label=1.0))\n",
    "            pid2 = rng.choice([x for x in persons if x != pid])\n",
    "            n1   = rng.choice(rows)\n",
    "            n2   = rng.choice(grouped.get_group(pid2).index.tolist())\n",
    "            examples.append(InputExample(\n",
    "                texts=[self.serial(df.loc[n1]), self.serial(df.loc[n2])], label=0.0))\n",
    "        loader = DataLoader(examples, shuffle=True, batch_size=32)\n",
    "        with Timer(f\"TriBERTa fit {out_dir.name}\"):\n",
    "            self.model.fit([(loader, self.loss)], epochs=1,\n",
    "                           show_progress_bar=False, output_path=str(out_dir))\n",
    "    def score(self, df, pairs, bs=BATCH_EMB):\n",
    "        sims = []\n",
    "        for i in tqdm(range(0,len(pairs),bs), desc=\"TriBERTa score\", leave=False):\n",
    "            chunk = pairs[i:i+bs]\n",
    "            a = [self.serial(df.loc[x]) for x,_ in chunk]\n",
    "            b = [self.serial(df.loc[y]) for _,y in chunk]\n",
    "            ea = self.model.encode(a, convert_to_tensor=True, device=DEVICE)\n",
    "            eb = self.model.encode(b, convert_to_tensor=True, device=DEVICE)\n",
    "            sims.append(torch.cosine_similarity(ea, eb).cpu().numpy())\n",
    "        return np.concatenate(sims)/2 + 0.5\n",
    "\n",
    "class EncodeDataset(Dataset):\n",
    "    def __init__(self, enc, y): self.enc, self.y = enc, y\n",
    "    def __getitem__(self, i):\n",
    "        d = {k:v[i] for k,v in self.enc.items()}; d[\"labels\"] = self.y[i]; return d\n",
    "    def __len__(self): return len(self.y)\n",
    "\n",
    "class DistilBERTER:\n",
    "    def __init__(self, name=\"distilbert-base-uncased\"):\n",
    "        self.tok = AutoTokenizer.from_pretrained(name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            name, num_labels=2).to(DEVICE)\n",
    "    def train(self, df, pairs, y, out_dir):\n",
    "        txt = [f\"A: {TriBERTaER.serial(df.loc[i])}\\nB: {TriBERTaER.serial(df.loc[j])}\"\n",
    "               for i,j in pairs]\n",
    "        enc = self.tok(txt, truncation=True, padding=True,\n",
    "                       max_length=256, return_tensors=\"pt\")\n",
    "        ds  = EncodeDataset(enc, torch.tensor(y))\n",
    "        args = TrainingArguments(output_dir=str(out_dir),\n",
    "                                per_device_train_batch_size=8,\n",
    "                                num_train_epochs=1, save_strategy=\"no\",\n",
    "                                fp16=True, remove_unused_columns=False,\n",
    "                                logging_steps=50)\n",
    "        with Timer(f\"DistilBERT fit {out_dir.name}\"):\n",
    "            Trainer(self.model, args, train_dataset=ds,\n",
    "                    tokenizer=self.tok).train()\n",
    "        self.model.save_pretrained(out_dir)\n",
    "    def score(self, df, pairs, bs=DISTIL_BS):\n",
    "        txt = [f\"A: {TriBERTaER.serial(df.loc[i])}\\nB: {TriBERTaER.serial(df.loc[j])}\"\n",
    "               for i,j in pairs]\n",
    "        out=[]\n",
    "        for i in tqdm(range(0,len(txt),bs), desc=\"Distil score\", leave=False):\n",
    "            enc=self.tok(txt[i:i+bs], truncation=True, padding=True,\n",
    "                         max_length=256, return_tensors=\"pt\").to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                logits=self.model(**enc).logits\n",
    "            out.append(torch.softmax(logits,-1)[:,1].cpu().numpy())\n",
    "        return np.concatenate(out)\n",
    "\n",
    "class MiniLMCE:\n",
    "    def __init__(self, name=\"sentence-transformers/paraphrase-MiniLM-L6-v2\"):\n",
    "        self.model = CrossEncoder(name, num_labels=1, device=DEVICE)\n",
    "    def train(self, df, pairs, y, out_dir):\n",
    "        samples = [InputExample(\n",
    "            texts=[TriBERTaER.serial(df.loc[i]),\n",
    "                   TriBERTaER.serial(df.loc[j])], label=float(l))\n",
    "            for (i,j), l in zip(pairs, y)]\n",
    "        loader = DataLoader(samples, shuffle=True, batch_size=16)\n",
    "        with Timer(f\"MiniLM‚ÄëCE fit {out_dir.name}\"):\n",
    "            self.model.fit(train_dataloader=loader, epochs=1)\n",
    "        self.model.save(out_dir)\n",
    "    def score(self, df, pairs, bs=MINILM_BS):\n",
    "        txt=[(TriBERTaER.serial(df.loc[i]),TriBERTaER.serial(df.loc[j]))\n",
    "             for i,j in pairs]\n",
    "        return np.array(self.model.predict(txt, batch_size=bs))\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ metrics / ckpt ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def metrics(y,p,t=0.5):\n",
    "    pred=(p>=t).astype(int)\n",
    "    pr,rc,f1,_=precision_recall_fscore_support(y,pred,average=\"binary\",zero_division=0)\n",
    "    return {\"precision\":pr,\"recall\":rc,\"f1\":f1,\"accuracy\":accuracy_score(y,pred),\n",
    "            \"auc\":roc_auc_score(y,p) if len(np.unique(y))>1 else float(\"nan\")}\n",
    "\n",
    "def ckpt_load(): return (json.loads(CKPT_FILE.read_text())[\"done_runs\"]\n",
    "                         if CKPT_FILE.exists() else 0)\n",
    "def ckpt_save(done,log):\n",
    "    CKPT_FILE.write_text(json.dumps({\"done_runs\":done}))\n",
    "    (OUTPUT/\"metrics_runs\").mkdir(exist_ok=True)\n",
    "    (OUTPUT/f\"metrics_runs/run_{done}.json\").write_text(json.dumps(log[-1],indent=2))\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ main ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "if __name__ == \"__main__\":\n",
    "    df = load_people()\n",
    "    df_lab = df[df[\"person\"].notna()].reset_index(drop=True)\n",
    "    blocks = build_blocks(df_lab)\n",
    "    collected = {m:[] for m in MODES}\n",
    "\n",
    "    for run in range(ckpt_load(), N_RUNS):\n",
    "        logging.info(\"=== RUN %d/%d ===\", run+1, N_RUNS)\n",
    "        rng=random.Random(RNG_SEED+run)\n",
    "        tr_idx, te_idx = train_test_split(df_lab.index.tolist(),\n",
    "                                          test_size=0.2,\n",
    "                                          random_state=run)\n",
    "\n",
    "        for mode in MODES:\n",
    "            with Timer(f\"pair sampling {mode}\"):\n",
    "                p_tr, p_te = sample_pairs(blocks, df_lab, tr_idx, te_idx,\n",
    "                                          MAX_TR, MAX_TE, mode, rng)\n",
    "            y_tr, y_te = labels_for(p_tr, df_lab), labels_for(p_te, df_lab)\n",
    "\n",
    "            tb = TriBERTaER();   tb_dir = OUTPUT/f\"{mode}_triberta_{run}\"\n",
    "            db = DistilBERTER(); db_dir = OUTPUT/f\"{mode}_distil_{run}\"\n",
    "            ce = MiniLMCE();     ce_dir = OUTPUT/f\"{mode}_minilm_{run}\"\n",
    "\n",
    "            tb.train(df_lab, tr_idx, tb_dir);          p_tb = tb.score(df_lab, p_te)\n",
    "            db.train(df_lab, p_tr, y_tr, db_dir);      p_db = db.score(df_lab, p_te)\n",
    "            ce.train(df_lab, p_tr, y_tr, ce_dir);      p_ce = ce.score(df_lab, p_te)\n",
    "\n",
    "            collected[mode].append({\"TriBERTa\":metrics(y_te,p_tb),\n",
    "                                    \"DistilBERT\":metrics(y_te,p_db),\n",
    "                                    \"MiniLM-CE\":metrics(y_te,p_ce)})\n",
    "\n",
    "            del tb, db, ce\n",
    "            torch.cuda.empty_cache(); gc.collect()\n",
    "\n",
    "        ckpt_save(run+1, collected)\n",
    "        logging.info(\"‚úî run %d finished\", run+1)\n",
    "\n",
    "    ks=[\"precision\",\"recall\",\"f1\",\"accuracy\",\"auc\"]\n",
    "    for mode in MODES:\n",
    "        avg={m:{k:np.mean([r[m][k] for r in collected[mode]]) for k in ks}\n",
    "             for m in (\"TriBERTa\",\"DistilBERT\",\"MiniLM-CE\")}\n",
    "        pd.DataFrame(avg).T.to_csv(OUTPUT/f\"{mode}_metrics_summary_avg.csv\")\n",
    "\n",
    "    logging.info(\"üéâ all runs complete\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ICE-ID-2.0-K4yMlCOc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
