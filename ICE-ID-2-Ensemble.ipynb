{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd45e2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from pathlib import Path\n",
    "from scipy import sparse\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Configuration\n",
    "# ---------------------------------------------------------------------------\n",
    "DATA_DIR   = Path(\"smb_data\")  # Input CSVs directory\n",
    "ART_DIR    = Path(\"artifacts\") # Output artifacts directory\n",
    "ART_DIR.mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560e6d81",
   "metadata": {},
   "source": [
    "# ENSEMBLE OF ML MODELS - DEPRECATED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a07cacf",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'smb_data/people.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 340\u001b[39m\n\u001b[32m    337\u001b[39m warnings.filterwarnings(\u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    338\u001b[39m logging.basicConfig(level=logging.INFO, \u001b[38;5;28mformat\u001b[39m=\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%(levelname)s\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%(message)s\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m340\u001b[39m X, labels, heimild = \u001b[43mload_X_labels_heimild\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    342\u001b[39m run_task(\u001b[33m\"\u001b[39m\u001b[33mwithin\u001b[39m\u001b[33m\"\u001b[39m , X, labels, heimild, SUBSET_DEF)\n\u001b[32m    343\u001b[39m run_task(\u001b[33m\"\u001b[39m\u001b[33macross\u001b[39m\u001b[33m\"\u001b[39m , X, labels, heimild, SUBSET_DEF)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 77\u001b[39m, in \u001b[36mload_X_labels_heimild\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     70\u001b[39m lbls = pd.read_csv(ART_DIR / \u001b[33m\"\u001b[39m\u001b[33mrow_labels.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     71\u001b[39m lbls[\u001b[33m\"\u001b[39m\u001b[33mperson\u001b[39m\u001b[33m\"\u001b[39m] = (\n\u001b[32m     72\u001b[39m     pd.to_numeric(lbls[\u001b[33m\"\u001b[39m\u001b[33mperson\u001b[39m\u001b[33m\"\u001b[39m], errors=\u001b[33m\"\u001b[39m\u001b[33mcoerce\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     73\u001b[39m       .fillna(-\u001b[32m1\u001b[39m)\n\u001b[32m     74\u001b[39m       .astype(\u001b[38;5;28mint\u001b[39m)\n\u001b[32m     75\u001b[39m )\n\u001b[32m     76\u001b[39m heimild_df = (\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msmb_data/people.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musecols\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mid\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mheimild\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m       .set_index(\u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     79\u001b[39m )\n\u001b[32m     80\u001b[39m heimild = (\n\u001b[32m     81\u001b[39m     heimild_df.reindex(lbls.row_id)[\u001b[33m\"\u001b[39m\u001b[33mheimild\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     82\u001b[39m               .fillna(-\u001b[32m1\u001b[39m)\n\u001b[32m     83\u001b[39m               .astype(\u001b[38;5;28mint\u001b[39m)\n\u001b[32m     84\u001b[39m               .values\n\u001b[32m     85\u001b[39m )\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m X, lbls[\u001b[33m\"\u001b[39m\u001b[33mperson\u001b[39m\u001b[33m\"\u001b[39m].values, heimild\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/ICE-ID-2.0-K4yMlCOc/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/ICE-ID-2.0-K4yMlCOc/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/ICE-ID-2.0-K4yMlCOc/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/ICE-ID-2.0-K4yMlCOc/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/ICE-ID-2.0-K4yMlCOc/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'smb_data/people.csv'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# train_ensemble_with_reports.py  ·  dual-task (within / across census)\n",
    "#                                  + streaming diagnostics to avoid OOM\n",
    "# -----------------------------------------------------------------------------\n",
    "# – trains 4 tree models + simple-average ensemble\n",
    "# – evaluates on “within-census” and “cross-census” record pairs\n",
    "# – full metrics / ROC / clustering diagnostics\n",
    "# – diagnostics scored in RAM-friendly batches (default 20 000 rows subsample)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "import itertools\n",
    "import json\n",
    "import joblib\n",
    "import logging\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from scipy import sparse\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import (\n",
    "    adjusted_rand_score,\n",
    "    accuracy_score,\n",
    "    auc,\n",
    "    f1_score,\n",
    "    precision_recall_fscore_support,\n",
    "    roc_curve,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# ───────────── paths / constants ─────────────\n",
    "ART_DIR      = Path(\"artifacts\")\n",
    "BASE_MODEL   = Path(\"models_ensemble\"); BASE_MODEL.mkdir(exist_ok=True)\n",
    "BASE_REPORT  = Path(\"reports\");         BASE_REPORT.mkdir(exist_ok=True)\n",
    "\n",
    "RNG_SEED     = 42\n",
    "NEG_PER_POS  = 2\n",
    "TOP_K        = 5\n",
    "SUBSET_DEF   = 1000        # default rows for diagnostics subsample\n",
    "\n",
    "# ───────────── helpers ─────────────\n",
    "class Timer:\n",
    "    def __init__(self, msg):\n",
    "        self.msg, self.t0 = msg, time.perf_counter()\n",
    "    def __enter__(self):\n",
    "        logging.info(f\"▶ {self.msg}\")\n",
    "        return self\n",
    "    def __exit__(self, *_):\n",
    "        logging.info(f\"⏱ {self.msg}: {time.perf_counter() - self.t0:.1f}s\")\n",
    "\n",
    "def prf_auc(y_true, y_prob):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    return {\"roc_auc\": auc(fpr, tpr)}, (fpr, tpr)\n",
    "\n",
    "# ───────────────────────────────────────\n",
    "#  Data loading\n",
    "# ───────────────────────────────────────\n",
    "def load_X_labels_heimild():\n",
    "    \"\"\"Load sparse feature matrix, person-ids and ‘heimild’ source labels.\"\"\"\n",
    "    X = sparse.load_npz(ART_DIR / \"iceid_ml_ready.npz\")\n",
    "    lbls = pd.read_csv(ART_DIR / \"row_labels.csv\")\n",
    "    lbls[\"person\"] = (\n",
    "        pd.to_numeric(lbls[\"person\"], errors=\"coerce\")\n",
    "          .fillna(-1)\n",
    "          .astype(int)\n",
    "    )\n",
    "    heimild_df = (\n",
    "        pd.read_csv(\"smb_data/people.csv\", usecols=[\"id\", \"heimild\"])\n",
    "          .set_index(\"id\")\n",
    "    )\n",
    "    heimild = (\n",
    "        heimild_df.reindex(lbls.row_id)[\"heimild\"]\n",
    "                  .fillna(-1)\n",
    "                  .astype(int)\n",
    "                  .values\n",
    "    )\n",
    "    return X, lbls[\"person\"].values, heimild\n",
    "\n",
    "# ───────────────────────────────────────\n",
    "#  Pair sampling\n",
    "# ───────────────────────────────────────\n",
    "def build_pairs(labels, heimild, mode):\n",
    "    idx_lab = [i for i, p in enumerate(labels) if p != -1]\n",
    "    id_by_person = {}\n",
    "    for i in idx_lab:\n",
    "        id_by_person.setdefault(labels[i], []).append(i)\n",
    "    # positives\n",
    "    pos = []\n",
    "    for grp in id_by_person.values():\n",
    "        for a, b in itertools.combinations(grp, 2):\n",
    "            same = heimild[a] == heimild[b]\n",
    "            if (mode == \"within\" and same) or (mode == \"across\" and not same):\n",
    "                pos.append((a, b))\n",
    "    # negatives\n",
    "    neg = set()\n",
    "    target = len(pos) * NEG_PER_POS\n",
    "    rng = random.Random(RNG_SEED)\n",
    "    while len(neg) < target:\n",
    "        a, b = rng.sample(idx_lab, 2)\n",
    "        if labels[a] == labels[b]:\n",
    "            continue\n",
    "        same = heimild[a] == heimild[b]\n",
    "        if (mode == \"within\" and same) or (mode == \"across\" and not same):\n",
    "            neg.add(tuple(sorted((a, b))))\n",
    "    return pos, list(neg)\n",
    "\n",
    "# ───────────────────────────────────────\n",
    "#  Sparse pair builder\n",
    "# ───────────────────────────────────────\n",
    "def pair_matrix(X, pairs):\n",
    "    return sparse.vstack([sparse.hstack([X[i], X[j]]) for i, j in pairs])\n",
    "\n",
    "# ───────────────────────────────────────\n",
    "#  Threshold tuning\n",
    "# ───────────────────────────────────────\n",
    "def tune_thr(y, p):\n",
    "    best_t, best_f = 0.5, 0.0\n",
    "    for t in np.linspace(0, 1, 101):\n",
    "        f = f1_score(y, p >= t)\n",
    "        if f > best_f:\n",
    "            best_f, best_t = f, t\n",
    "    return best_t\n",
    "\n",
    "# ───────────────────────────────────────\n",
    "#  Streaming diagnostics helpers\n",
    "# ───────────────────────────────────────\n",
    "def batched_pairs(idx_rows, batch_sz=10_000):\n",
    "    combos = itertools.combinations(range(len(idx_rows)), 2)\n",
    "    while True:\n",
    "        batch = list(itertools.islice(combos, batch_sz))\n",
    "        if not batch:\n",
    "            break\n",
    "        yield batch\n",
    "\n",
    "def score_pairs_stream(X, idx_rows, ens_fn, batch_sz=10_000):\n",
    "    \"\"\"Compute ensemble scores in streaming batches with progress bar.\"\"\"\n",
    "    scores = []\n",
    "    total_pairs = len(idx_rows) * (len(idx_rows)-1) // 2\n",
    "    # approximate number of batches\n",
    "    n_batches = (total_pairs + batch_sz - 1)//batch_sz\n",
    "    for chunk in tqdm(\n",
    "        batched_pairs(idx_rows, batch_sz),\n",
    "        total=n_batches,\n",
    "        desc=\"diagnostics batches\",\n",
    "        unit=\"batch\"\n",
    "    ):\n",
    "        mat = pair_matrix(X, [(idx_rows[i], idx_rows[j]) for i, j in chunk])\n",
    "        scores.extend(ens_fn(mat))\n",
    "    return np.asarray(scores, dtype=np.float32)\n",
    "\n",
    "# ───────────────────────────────────────\n",
    "#  Diagnostics metrics\n",
    "# ───────────────────────────────────────\n",
    "def clustering_cc(labels, probs, thr):\n",
    "    n = len(labels)\n",
    "    parent = list(range(n))\n",
    "    def find(x):\n",
    "        while parent[x] != x:\n",
    "            parent[x] = parent[parent[x]]\n",
    "            x = parent[x]\n",
    "        return x\n",
    "    k = 0\n",
    "    for i, j in itertools.combinations(range(n), 2):\n",
    "        if probs[k] >= thr:\n",
    "            parent[find(i)] = find(j)\n",
    "        k += 1\n",
    "    clusters = [find(i) for i in range(n)]\n",
    "    gold = [l if l != -1 else -1 for l in labels]\n",
    "    return adjusted_rand_score(gold, clusters)\n",
    "\n",
    "def clustering_aggl(labels, probs, n_clusters):\n",
    "    n = len(labels)\n",
    "    P = np.zeros((n, n))\n",
    "    k = 0\n",
    "    for i, j in itertools.combinations(range(n), 2):\n",
    "        P[i,j] = P[j,i] = probs[k]\n",
    "        k += 1\n",
    "    D = 1 - P\n",
    "    pred = AgglomerativeClustering(\n",
    "        n_clusters=n_clusters, metric=\"precomputed\", linkage=\"average\"\n",
    "    ).fit_predict(D)\n",
    "    gold = [l if l != -1 else -1 for l in labels]\n",
    "    return adjusted_rand_score(gold, pred)\n",
    "\n",
    "def retrieval_at_k(labels, probs, k=TOP_K):\n",
    "    n = len(labels)\n",
    "    P = np.zeros((n, n))\n",
    "    idx = 0\n",
    "    for i, j in itertools.combinations(range(n), 2):\n",
    "        P[i,j] = P[j,i] = probs[idx]\n",
    "        idx += 1\n",
    "    correct = 0\n",
    "    total_true = sum(\n",
    "        1 for i in range(n) for j in range(n)\n",
    "        if i != j and labels[i] == labels[j] != -1\n",
    "    )\n",
    "    retrieved = 0\n",
    "    for i in range(n):\n",
    "        topk = np.argsort(P[i])[::-1][:k]\n",
    "        retrieved += k\n",
    "        correct += sum(1 for j in topk if labels[i] == labels[j] != -1)\n",
    "    precision = correct / retrieved if retrieved else 0\n",
    "    recall    = correct / total_true if total_true else 0\n",
    "    return precision, recall\n",
    "\n",
    "# ───────────────────────────────────────\n",
    "#  Pipeline per task\n",
    "# ───────────────────────────────────────\n",
    "def run_task(mode, X, labels, heimild, subset_sz=SUBSET_DEF):\n",
    "    tag = mode\n",
    "    model_dir  = BASE_MODEL  / tag; model_dir.mkdir(exist_ok=True)\n",
    "    report_dir = BASE_REPORT / tag; report_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    logging.info(\"=== %s ===\", tag)\n",
    "    pos, neg = build_pairs(labels, heimild, mode)\n",
    "    if not pos:\n",
    "        logging.warning(\"no positives for %s\", tag)\n",
    "        return\n",
    "\n",
    "    # build train/test pairs\n",
    "    X_pos = pair_matrix(X, pos)\n",
    "    X_neg = pair_matrix(X, neg)\n",
    "    y_pos = np.ones(len(pos), int)\n",
    "    y_neg = np.zeros(len(neg), int)\n",
    "    X_pairs = sparse.vstack([X_pos, X_neg])\n",
    "    y_pairs = np.concatenate([y_pos, y_neg])\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "        X_pairs, y_pairs, stratify=y_pairs, test_size=0.2, random_state=RNG_SEED\n",
    "    )\n",
    "\n",
    "    # initialize models\n",
    "    models = {\n",
    "        \"xgb\": XGBClassifier(\n",
    "            tree_method=\"hist\", eval_metric=\"logloss\",\n",
    "            n_estimators=300, random_state=RNG_SEED\n",
    "        ),\n",
    "        \"lgb\": LGBMClassifier(\n",
    "            n_estimators=300, random_state=RNG_SEED, verbosity=-1\n",
    "        ),\n",
    "        \"cat\": CatBoostClassifier(\n",
    "            iterations=300, depth=6, learning_rate=0.1,\n",
    "            random_state=RNG_SEED, verbose=False\n",
    "        ),\n",
    "        \"rf\": RandomForestClassifier(\n",
    "            n_estimators=300, n_jobs=-1, random_state=RNG_SEED\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    # train models\n",
    "    with Timer(f\"fit-{tag}\"):\n",
    "        for name, mdl in tqdm(models.items(), desc=f\"fit-{tag}\", unit=\"model\"):\n",
    "            mdl.fit(X_tr, y_tr)\n",
    "            joblib.dump(mdl, model_dir / f\"{name}.pkl\")\n",
    "\n",
    "    # ensemble prediction function\n",
    "    ens_prob = lambda M: np.column_stack(\n",
    "        [m.predict_proba(M)[:,1] for m in models.values()]\n",
    "    ).mean(1)\n",
    "\n",
    "    # evaluate on hold-out\n",
    "    y_prob = ens_prob(X_te)\n",
    "    thr    = tune_thr(y_te, y_prob)\n",
    "    preds  = y_prob >= thr\n",
    "\n",
    "    pr, rc, f1, _ = precision_recall_fscore_support(\n",
    "        y_te, preds, average=\"binary\", zero_division=0\n",
    "    )\n",
    "    acc           = accuracy_score(y_te, preds)\n",
    "    fpr, tpr, _   = roc_curve(y_te, y_prob)\n",
    "    auc_val       = auc(fpr, tpr)\n",
    "\n",
    "    # write metrics\n",
    "    metrics = dict(\n",
    "        precision=pr, recall=rc, f1=f1,\n",
    "        accuracy=acc, auc=auc_val, threshold=thr\n",
    "    )\n",
    "    (report_dir / \"metrics.json\").write_text(json.dumps(metrics, indent=2))\n",
    "    pd.DataFrame([metrics]).to_csv(\n",
    "        report_dir / \"metrics_summary.csv\", index=False\n",
    "    )\n",
    "\n",
    "    # plot ROC\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=f\"AUC={auc_val:.3f}\")\n",
    "    plt.plot([0,1], [0,1], \"--\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"ROC – {tag}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(report_dir / \"roc.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # ─────── diagnostics ───────\n",
    "    subset_idx = np.random.default_rng(RNG_SEED).choice(\n",
    "        len(labels), size=min(subset_sz, len(labels)), replace=False\n",
    "    )\n",
    "    logging.info(\"%s diagnostics on %d rows\", tag, len(subset_idx))\n",
    "    probs_sub = score_pairs_stream(\n",
    "        X, subset_idx, ens_prob, batch_sz=100_000\n",
    "    )\n",
    "\n",
    "    y_sub = labels[subset_idx]\n",
    "    ari_cc = clustering_cc(y_sub, probs_sub, thr)\n",
    "    ari_ag = clustering_aggl(\n",
    "        y_sub, probs_sub, len(np.unique(y_sub[y_sub != -1]))\n",
    "    )\n",
    "    p_k, r_k = retrieval_at_k(y_sub, probs_sub)\n",
    "\n",
    "    diag = dict(\n",
    "        ari_cc=ari_cc, ari_ag=ari_ag,\n",
    "        precision_at_k=p_k, recall_at_k=r_k\n",
    "    )\n",
    "    (report_dir / \"diagnostics.json\").write_text(json.dumps(diag, indent=2))\n",
    "\n",
    "    # plot cluster sizes\n",
    "    plt.figure()\n",
    "    pd.Series(y_sub[y_sub != -1]).value_counts().hist(bins=50)\n",
    "    plt.title(\"Cluster size distribution\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(report_dir / \"cluster_size_dist.png\")\n",
    "    plt.close()\n",
    "\n",
    "    logging.info(\"✔ %s done – results in %s\", tag, report_dir)\n",
    "\n",
    "# ───────────────────────────────────────\n",
    "#  Main entrypoint\n",
    "# ───────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    logging.basicConfig(level=logging.INFO, format=\"%(levelname)s %(message)s\")\n",
    "\n",
    "    X, labels, heimild = load_X_labels_heimild()\n",
    "\n",
    "    run_task(\"within\" , X, labels, heimild, SUBSET_DEF)\n",
    "    run_task(\"across\" , X, labels, heimild, SUBSET_DEF)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f6adda",
   "metadata": {},
   "source": [
    "# FINAL ENSEMBLE TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a71cef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 22:37:45 [INFO] PyTorch 2.7.0+cu126; CUDA=yes\n",
      "2025-05-11 22:37:45 [INFO] Loading features + metadata for training...\n",
      "2025-05-11 22:37:46 [INFO] Loaded X (984028, 50799), labels 984028, heimild len 984028\n",
      "2025-05-11 22:37:47 [INFO] Age‐disp threshold=76\n",
      "2025-05-11 22:37:52 [INFO] ▶ [run 0] Fitting models for within\n",
      "2025-05-11 22:39:11 [INFO] ⏱ [run 0] Fitting models for within: 78.6s\n",
      "2025-05-11 22:40:45 [INFO] ▶ [run 0] Fitting models for across\n",
      "2025-05-11 23:00:12 [INFO] ⏱ [run 0] Fitting models for across: 1166.8s\n",
      "2025-05-11 23:00:43 [INFO] Age‐disp threshold=76\n",
      "2025-05-11 23:00:48 [INFO] ▶ [run 1] Fitting models for within\n",
      "2025-05-11 23:02:15 [INFO] ⏱ [run 1] Fitting models for within: 86.4s\n",
      "2025-05-11 23:03:44 [INFO] ▶ [run 1] Fitting models for across\n",
      "2025-05-11 23:23:04 [INFO] ⏱ [run 1] Fitting models for across: 1159.6s\n",
      "2025-05-11 23:23:35 [INFO] Age‐disp threshold=76\n",
      "2025-05-11 23:23:40 [INFO] ▶ [run 2] Fitting models for within\n",
      "2025-05-11 23:25:02 [INFO] ⏱ [run 2] Fitting models for within: 82.0s\n",
      "2025-05-11 23:26:30 [INFO] ▶ [run 2] Fitting models for across\n",
      "2025-05-11 23:44:32 [INFO] ⏱ [run 2] Fitting models for across: 1081.4s\n",
      "2025-05-11 23:45:01 [INFO] Age‐disp threshold=76\n",
      "2025-05-11 23:45:06 [INFO] ▶ [run 3] Fitting models for within\n",
      "2025-05-11 23:46:19 [INFO] ⏱ [run 3] Fitting models for within: 73.5s\n",
      "2025-05-11 23:47:45 [INFO] ▶ [run 3] Fitting models for across\n",
      "2025-05-12 00:04:42 [INFO] ⏱ [run 3] Fitting models for across: 1017.0s\n",
      "2025-05-12 00:05:11 [INFO] Age‐disp threshold=76\n",
      "2025-05-12 00:05:16 [INFO] ▶ [run 4] Fitting models for within\n",
      "2025-05-12 00:06:29 [INFO] ⏱ [run 4] Fitting models for within: 73.0s\n",
      "2025-05-12 00:07:54 [INFO] ▶ [run 4] Fitting models for across\n",
      "2025-05-12 00:24:55 [INFO] ⏱ [run 4] Fitting models for across: 1020.5s\n",
      "2025-05-12 00:25:24 [INFO] Age‐disp threshold=76\n",
      "2025-05-12 00:25:29 [INFO] ▶ [run 5] Fitting models for within\n",
      "2025-05-12 00:26:42 [INFO] ⏱ [run 5] Fitting models for within: 73.2s\n",
      "2025-05-12 00:28:07 [INFO] ▶ [run 5] Fitting models for across\n",
      "2025-05-12 00:45:02 [INFO] ⏱ [run 5] Fitting models for across: 1015.2s\n",
      "2025-05-12 00:45:31 [INFO] Age‐disp threshold=76\n",
      "2025-05-12 00:45:36 [INFO] ▶ [run 6] Fitting models for within\n",
      "2025-05-12 00:46:48 [INFO] ⏱ [run 6] Fitting models for within: 71.7s\n",
      "2025-05-12 00:48:13 [INFO] ▶ [run 6] Fitting models for across\n",
      "2025-05-12 01:05:07 [INFO] ⏱ [run 6] Fitting models for across: 1013.1s\n",
      "2025-05-12 01:05:36 [INFO] Age‐disp threshold=76\n",
      "2025-05-12 01:05:40 [INFO] ▶ [run 7] Fitting models for within\n",
      "2025-05-12 01:06:54 [INFO] ⏱ [run 7] Fitting models for within: 73.2s\n",
      "2025-05-12 01:08:19 [INFO] ▶ [run 7] Fitting models for across\n",
      "2025-05-12 01:25:08 [INFO] ⏱ [run 7] Fitting models for across: 1008.9s\n",
      "2025-05-12 01:25:37 [INFO] Age‐disp threshold=76\n",
      "2025-05-12 01:25:42 [INFO] ▶ [run 8] Fitting models for within\n",
      "2025-05-12 01:26:54 [INFO] ⏱ [run 8] Fitting models for within: 71.9s\n",
      "2025-05-12 01:28:20 [INFO] ▶ [run 8] Fitting models for across\n",
      "2025-05-12 01:45:16 [INFO] ⏱ [run 8] Fitting models for across: 1015.8s\n",
      "2025-05-12 01:45:45 [INFO] Age‐disp threshold=76\n",
      "2025-05-12 01:45:50 [INFO] ▶ [run 9] Fitting models for within\n",
      "2025-05-12 01:47:02 [INFO] ⏱ [run 9] Fitting models for within: 72.5s\n",
      "2025-05-12 01:48:28 [INFO] ▶ [run 9] Fitting models for across\n",
      "2025-05-12 02:05:19 [INFO] ⏱ [run 9] Fitting models for across: 1011.4s\n",
      "2025-05-12 02:05:48 [INFO] ✔ All runs complete; per-mode and combined averaged metrics written to reports/\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# train_ensemble_gpu.py  ·  dual-task (within / across census)\n",
    "#                          + GPU support + age disparity calculation\n",
    "#                          + repeated random-sampling runs + averaged results\n",
    "# -----------------------------------------------------------------------------\n",
    "# – trains 4 tree models + simple-average ensemble\n",
    "# – runs end-to-end pipeline 10 times with new random pairs each run\n",
    "# – evaluates on fresh train/test split each run\n",
    "# – diagnostics on held-out records each run\n",
    "# – clears memory between runs\n",
    "# – averages all metrics across runs and saves final averages\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "import itertools\n",
    "import json\n",
    "import joblib\n",
    "import logging\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import gc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from sklearn.datasets import dump_svmlight_file\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    adjusted_rand_score,\n",
    "    accuracy_score,\n",
    "    auc,\n",
    "    f1_score,\n",
    "    precision_recall_fscore_support,\n",
    "    roc_curve,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Attempt to import GPU-accelerated libraries\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    from xgboost import XGBClassifier\n",
    "except ImportError:\n",
    "    logging.warning(\"XGBoost not found. XGBoost models will not be trained.\")\n",
    "    XGBClassifier = None\n",
    "try:\n",
    "    from lightgbm import LGBMClassifier\n",
    "except ImportError:\n",
    "    logging.warning(\"LightGBM not found. LightGBM models will not be trained.\")\n",
    "    LGBMClassifier = None\n",
    "try:\n",
    "    from catboost import CatBoostClassifier\n",
    "except ImportError:\n",
    "    logging.warning(\"CatBoost not found. CatBoost models will not be trained.\")\n",
    "    CatBoostClassifier = None\n",
    "try:\n",
    "    import torch  # For CUDA check\n",
    "except ImportError:\n",
    "    torch = None\n",
    "    logging.warning(\"PyTorch not installed. CUDA availability check might be limited.\")\n",
    "\n",
    "\n",
    "# ───────────── paths / constants ─────────────\n",
    "DATA_DIR       = Path(\"raw_data\")\n",
    "ART_DIR        = Path(\"artifacts\")\n",
    "BASE_MODEL_DIR = Path(\"models_ensemble_gpu\"); BASE_MODEL_DIR.mkdir(exist_ok=True, parents=True)\n",
    "BASE_REPORT    = Path(\"reports_gpu\");         BASE_REPORT.mkdir(exist_ok=True, parents=True)\n",
    "ART_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "RNG_SEED                = 42\n",
    "NEG_PER_POS             = 2\n",
    "TOP_K                   = 5\n",
    "AGE_DISPARITY_PERCENTILE = 95\n",
    "\n",
    "# Caps & diagnostics batching\n",
    "MAX_TOTAL_PAIRS         = 500_000    # cap on training pairs\n",
    "MAX_DIAG_PAIRS          = 100_000    # cap on diagnostics pairs per run\n",
    "DIAG_CLUSTER_SUBSET_SZ  = 2000       # clustering diag batch size\n",
    "DIAG_CLUSTER_BATCHES    = 10         # clustering diag num batches\n",
    "\n",
    "# Number of repeated runs\n",
    "NUM_RUNS = 10\n",
    "\n",
    "\n",
    "# ───────────── helpers ─────────────\n",
    "class Timer:\n",
    "    def __init__(self, msg):\n",
    "        self.msg, self.t0 = msg, time.perf_counter()\n",
    "    def __enter__(self):\n",
    "        logging.info(f\"▶ {self.msg}\")\n",
    "        return self\n",
    "    def __exit__(self, *_):\n",
    "        logging.info(f\"⏱ {self.msg}: {time.perf_counter() - self.t0:.1f}s\")\n",
    "\n",
    "\n",
    "def pair_matrix(X, pairs):\n",
    "    \"\"\"Builds [X[i] | X[j]] for each (i,j).\"\"\"\n",
    "    if not pairs:\n",
    "        return sparse.csr_matrix((0, X.shape[1]*2), dtype=X.dtype)\n",
    "    return sparse.vstack([\n",
    "        sparse.hstack([X[i], X[j]]) for i,j in pairs\n",
    "    ])\n",
    "\n",
    "\n",
    "def tune_thr(y_true, y_prob):\n",
    "    best_t, best_f1 = 0.5, 0.0\n",
    "    for t in np.linspace(0,1,101):\n",
    "        f1 = f1_score(y_true, y_prob>=t, zero_division=0)\n",
    "        if f1>best_f1:\n",
    "            best_f1, best_t = f1, t\n",
    "    return best_t\n",
    "\n",
    "\n",
    "# ───────────────────────────────────────\n",
    "#  Data loading & Age threshold\n",
    "# ───────────────────────────────────────\n",
    "def load_training_data_with_details():\n",
    "    logging.info(\"Loading features + metadata for training...\")\n",
    "    X_path = ART_DIR/\"iceid_ml_ready.npz\"\n",
    "    X = sparse.load_npz(X_path)\n",
    "    rows_df = pd.read_csv(ART_DIR/\"row_labels.csv\")\n",
    "    rows_df[\"person\"] = pd.to_numeric(rows_df[\"person\"],errors=\"coerce\").fillna(-1).astype(int)\n",
    "    labels      = rows_df[\"person\"].values\n",
    "    orig_ids    = rows_df[\"row_id\"].values\n",
    "\n",
    "    people_df = pd.read_csv(\n",
    "        DATA_DIR/\"people.csv\",\n",
    "        usecols=[\"id\",\"heimild\",\"birthyear\"],\n",
    "        dtype={\"id\":orig_ids.dtype}\n",
    "    ).set_index(\"id\")\n",
    "    aligned    = people_df.reindex(orig_ids)\n",
    "    heimild    = aligned[\"heimild\"].fillna(-1).astype(int).values\n",
    "    birthyrs   = pd.to_numeric(aligned[\"birthyear\"],errors=\"coerce\").fillna(0).astype(int).values\n",
    "\n",
    "    logging.info(f\"Loaded X {X.shape}, labels {len(labels)}, heimild len {len(heimild)}\")\n",
    "    return X, labels, heimild, birthyrs\n",
    "\n",
    "\n",
    "def calculate_and_save_age_disparity_threshold(labels, birthyrs, out_dir):\n",
    "    p2b = {}\n",
    "    for i,p in enumerate(labels):\n",
    "        by = birthyrs[i]\n",
    "        if p!=-1 and by>1000:\n",
    "            p2b.setdefault(p,[]).append(by)\n",
    "    spans = [(max(bs)-min(bs)) if len(bs)>1 else 0 for bs in p2b.values()]\n",
    "    if not spans:\n",
    "        thr=10\n",
    "    else:\n",
    "        thr=int(np.percentile(spans,AGE_DISPARITY_PERCENTILE))\n",
    "        if thr<1 and AGE_DISPARITY_PERCENTILE>=90 and any(s>0 for s in spans):\n",
    "            thr=1\n",
    "    out_dir.mkdir(exist_ok=True, parents=True)\n",
    "    (out_dir/\"age_disparity_threshold.json\").write_text(\n",
    "        json.dumps({\"age_disparity_threshold_years\":thr})\n",
    "    )\n",
    "    logging.info(f\"Age‐disp threshold={thr}\")\n",
    "    return thr\n",
    "\n",
    "\n",
    "# ───────────────────────────────────────\n",
    "#  Pair sampling\n",
    "# ───────────────────────────────────────\n",
    "def build_pairs_for_training(labels, heimild, mode, seed):\n",
    "    rng = random.Random(seed)\n",
    "    idx_lab = [i for i,p in enumerate(labels) if p!=-1]\n",
    "    by_person={}\n",
    "    for i in idx_lab:\n",
    "        by_person.setdefault(labels[i],[]).append(i)\n",
    "\n",
    "    pos=[]\n",
    "    for inds in by_person.values():\n",
    "        for a,b in itertools.combinations(inds,2):\n",
    "            if heimild[a]==-1 or heimild[b]==-1: continue\n",
    "            same=(heimild[a]==heimild[b])\n",
    "            if mode==\"within\" and same: pos.append((a,b))\n",
    "            if mode==\"across\" and not same: pos.append((a,b))\n",
    "\n",
    "    target_neg = len(pos)*NEG_PER_POS\n",
    "    neg_set=set()\n",
    "    attempts,max_attempts=0,target_neg*20+1000\n",
    "    while len(neg_set)<target_neg and attempts<max_attempts:\n",
    "        attempts+=1\n",
    "        a,b = rng.sample(idx_lab,2)\n",
    "        if labels[a]==labels[b] or heimild[a]==-1 or heimild[b]==-1: continue\n",
    "        same=(heimild[a]==heimild[b])\n",
    "        if mode==\"within\" and same: neg_set.add(tuple(sorted((a,b))))\n",
    "        if mode==\"across\" and not same: neg_set.add(tuple(sorted((a,b))))\n",
    "\n",
    "    pos_final=pos if len(pos)+len(neg_set)<=MAX_TOTAL_PAIRS else rng.sample(pos,MAX_TOTAL_PAIRS//2)\n",
    "    neg_final=list(neg_set) if len(pos)+len(neg_set)<=MAX_TOTAL_PAIRS else rng.sample(list(neg_set),MAX_TOTAL_PAIRS//2)\n",
    "    return pos_final, neg_final\n",
    "\n",
    "\n",
    "def sample_diag_pairs(indices, heimild, mode, n_pairs, seed):\n",
    "    rng, pairs = random.Random(seed), set()\n",
    "    attempts, max_attempts = 0, n_pairs*20+1000\n",
    "    while len(pairs)<n_pairs and attempts<max_attempts:\n",
    "        attempts+=1\n",
    "        a,b = rng.sample(indices,2)\n",
    "        if heimild[a]==-1 or heimild[b]==-1: continue\n",
    "        same=(heimild[a]==heimild[b])\n",
    "        if mode==\"within\" and not same: continue\n",
    "        if mode==\"across\" and same: continue\n",
    "        pairs.add(tuple(sorted((a,b))))\n",
    "    return list(pairs)\n",
    "\n",
    "\n",
    "# ───────────────────────────────────────\n",
    "#  Batched diagnostics averaging\n",
    "# ───────────────────────────────────────\n",
    "def _diag_batch_metrics(batch_ids, pairs, scores, thr, labels):\n",
    "    # same as before: CC, AG, P@K, R@K on just batch_ids subset\n",
    "    local_map={rid:i for i,rid in enumerate(batch_ids)}\n",
    "    N=len(batch_ids)\n",
    "    gold=[labels[r] for r in batch_ids]\n",
    "    # treat unknowns\n",
    "    neg=-2\n",
    "    for i,v in enumerate(gold):\n",
    "        if v==-1: gold[i]=neg; neg-=1\n",
    "    # CC\n",
    "    parent=list(range(N))\n",
    "    def find(x):\n",
    "        if parent[x]!=x: parent[x]=find(parent[x])\n",
    "        return parent[x]\n",
    "    def union(a,b):\n",
    "        ra,rb=find(a),find(b)\n",
    "        if ra!=rb: parent[ra]=rb\n",
    "    for (a,b),s in zip(pairs,scores):\n",
    "        if a in local_map and b in local_map and s>=thr:\n",
    "            union(local_map[a],local_map[b])\n",
    "    pred_cc=[find(i) for i in range(N)]\n",
    "    ari_cc=adjusted_rand_score(gold,pred_cc)\n",
    "    # AG\n",
    "    ari_ag=float('nan')\n",
    "    true_clusters=len({g for g in gold if g>=0})\n",
    "    if true_clusters==0: true_clusters=N\n",
    "    if 2<=N and 1<=true_clusters<=N:\n",
    "        sim=np.zeros((N,N),dtype=np.float32)\n",
    "        for (a,b),s in zip(pairs,scores):\n",
    "            if a in local_map and b in local_map:\n",
    "                i,j=local_map[a],local_map[b]\n",
    "                sim[i,j]=sim[j,i]=s\n",
    "        dist=1-sim; np.fill_diagonal(dist,0)\n",
    "        try:\n",
    "            agg=AgglomerativeClustering(n_clusters=true_clusters,\n",
    "                                        metric=\"precomputed\",\n",
    "                                        linkage=\"average\")\n",
    "            pred_ag=agg.fit_predict(dist)\n",
    "            ari_ag=adjusted_rand_score(gold,pred_ag)\n",
    "        except:\n",
    "            ari_ag=float('nan')\n",
    "    # Retrieval\n",
    "    prob=np.full((N,N),-1.0,dtype=np.float32)\n",
    "    for (a,b),s in zip(pairs,scores):\n",
    "        if a in local_map and b in local_map:\n",
    "            i,j=local_map[a],local_map[b]\n",
    "            prob[i,j]=prob[j,i]=s\n",
    "    correct=0;retr=0;rel=0\n",
    "    for i in range(N):\n",
    "        lbl=labels[batch_ids[i]]\n",
    "        if lbl==-1: continue\n",
    "        # relevant\n",
    "        r_count=sum(1 for j in range(N)\n",
    "                    if j!=i and labels[batch_ids[j]]==lbl)\n",
    "        rel+=r_count\n",
    "        # top-K\n",
    "        neigh=[(prob[i,j],j) for j in range(N) if prob[i,j]>-0.5]\n",
    "        neigh.sort(key=lambda x:x[0],reverse=True)\n",
    "        for s,j in neigh[:TOP_K]:\n",
    "            retr+=1\n",
    "            if labels[batch_ids[j]]==lbl: correct+=1\n",
    "    rel/=2\n",
    "    p_at_k=correct/retr if retr else 0.0\n",
    "    r_at_k=correct/rel if rel else 1.0\n",
    "\n",
    "    return ari_cc, ari_ag, p_at_k, r_at_k\n",
    "\n",
    "\n",
    "def batched_diagnostics(pairs, scores, thr, heldout, labels):\n",
    "    rng=random.Random(RNG_SEED)\n",
    "    results=Parallel(n_jobs=-1)(\n",
    "        delayed(_diag_batch_metrics)(\n",
    "            rng.sample(heldout,min(DIAG_CLUSTER_SUBSET_SZ,len(heldout))),\n",
    "            pairs,scores,thr,labels\n",
    "        ) for _ in range(DIAG_CLUSTER_BATCHES)\n",
    "    )\n",
    "    arr=np.array(results,dtype=np.float32)  # (batches,4)\n",
    "    return tuple(arr.mean(axis=0).tolist())\n",
    "\n",
    "\n",
    "# ───────────────────────────────────────\n",
    "#  Main pipeline per run\n",
    "# ───────────────────────────────────────\n",
    "def run_once(run_id, X, labels, heimild, birthyrs, heldout):\n",
    "    \"\"\"\n",
    "    Execute one full pipeline: sample pairs, train, test, diagnose.\n",
    "    Returns dict of metrics.\n",
    "    \"\"\"\n",
    "    np.random.seed(RNG_SEED+run_id)\n",
    "    random.seed(RNG_SEED+run_id)\n",
    "\n",
    "    # Age threshold (constant across runs)\n",
    "    age_thr = calculate_and_save_age_disparity_threshold(\n",
    "        labels, birthyrs, BASE_REPORT/\"_global_training_outputs\"\n",
    "    )\n",
    "\n",
    "    all_results={}\n",
    "    for mode in (\"within\",\"across\"):\n",
    "        # 1) sample train pairs\n",
    "        pos,neg = build_pairs_for_training(labels,heimild,mode,seed=RNG_SEED+run_id)\n",
    "        Xp=pair_matrix(X,pos); Xn=pair_matrix(X,neg)\n",
    "        yp=np.ones(len(pos),int); yn=np.zeros(len(neg),int)\n",
    "        Xall=sparse.vstack([Xp,Xn]); yall=np.concatenate([yp,yn])\n",
    "        Xtr,Xte,ytr,yte = train_test_split(\n",
    "            Xall,yall,stratify=yall,test_size=0.2,random_state=RNG_SEED+run_id\n",
    "        )\n",
    "\n",
    "        # 2) train ensemble\n",
    "        models={}\n",
    "        if XGBClassifier:\n",
    "            models[\"xgb\"]=XGBClassifier(\n",
    "                tree_method=\"hist\",\n",
    "                device=\"cuda\" if torch and torch.cuda.is_available() else \"cpu\",\n",
    "                eval_metric=\"logloss\",n_estimators=300,random_state=RNG_SEED+run_id\n",
    "            )\n",
    "        if LGBMClassifier:\n",
    "            models[\"lgb\"]=LGBMClassifier(\n",
    "                n_estimators=300,random_state=RNG_SEED+run_id,verbosity=-1,\n",
    "                device=\"gpu\" if torch and torch.cuda.is_available() else \"cpu\"\n",
    "            )\n",
    "        if CatBoostClassifier:\n",
    "            models[\"cat\"]=CatBoostClassifier(\n",
    "                iterations=300,depth=6,learning_rate=0.1,\n",
    "                random_state=RNG_SEED+run_id,verbose=False,\n",
    "                task_type=\"GPU\" if torch and torch.cuda.is_available() else \"CPU\"\n",
    "            )\n",
    "        models[\"rf\"]=RandomForestClassifier(n_estimators=300,n_jobs=-1,random_state=RNG_SEED+run_id)\n",
    "\n",
    "        trained={}\n",
    "        with Timer(f\"[run {run_id}] Fitting models for {mode}\"):\n",
    "            for name,mdl in models.items():\n",
    "                try:\n",
    "                    if name==\"xgb\":\n",
    "                        tmp=BASE_MODEL_DIR/f\"{mode}_run{run_id}_xgb.libsvm\"\n",
    "                        dump_svmlight_file(Xtr,ytr,str(tmp))\n",
    "                        dtrain=xgb.DMatrix(f\"{tmp}?format=libsvm#dtrain.cache\")\n",
    "                        params={\"objective\":\"binary:logistic\",\"tree_method\":\"gpu_hist\",\n",
    "                                \"eval_metric\":\"logloss\",\"seed\":RNG_SEED+run_id}\n",
    "                        bst=xgb.train(params,dtrain,num_boost_round=300)\n",
    "                        trained[\"xgb\"]=bst\n",
    "                    else:\n",
    "                        mdl.fit(Xtr,ytr); trained[name]=mdl\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Run {run_id} {mode} {name} train error: {e}\")\n",
    "\n",
    "        def _pred(m, M):\n",
    "            if isinstance(m,xgb.Booster):\n",
    "                return m.predict(xgb.DMatrix(M))\n",
    "            else:\n",
    "                return m.predict_proba(M)[:,1]\n",
    "        ensemble_fn=lambda M: np.column_stack([_pred(m,M) for m in trained.values()]).mean(axis=1)\n",
    "\n",
    "        # 3) evaluate\n",
    "        p_te=ensemble_fn(Xte)\n",
    "        thr = tune_thr(yte,p_te)\n",
    "        pr,rc,f1,_=precision_recall_fscore_support(yte,p_te>=thr,average=\"binary\",zero_division=0)\n",
    "        acc=accuracy_score(yte,p_te>=thr)\n",
    "        try:\n",
    "            fpr,tpr,_=roc_curve(yte,p_te); aucv=auc(fpr,tpr)\n",
    "        except: aucv=float('nan')\n",
    "        all_results[f\"{mode}_test\"]={\"precision\":pr,\"recall\":rc,\"f1\":f1,\n",
    "                                     \"accuracy\":acc,\"threshold\":thr,\"auc\":aucv}\n",
    "\n",
    "        # 4) diagnostics\n",
    "        diag_pairs=sample_diag_pairs(heldout,heimild,mode,MAX_DIAG_PAIRS,seed=RNG_SEED+run_id)\n",
    "        diag_scores=[]\n",
    "        B=10_000\n",
    "        for i in range(0,len(diag_pairs),B):\n",
    "            sub=diag_pairs[i:i+B]\n",
    "            M=pair_matrix(X,sub)\n",
    "            diag_scores.extend(ensemble_fn(M))\n",
    "        ari_cc,ari_ag,p_at_k,r_at_k = batched_diagnostics(\n",
    "            diag_pairs,diag_scores,thr,heldout,labels\n",
    "        )\n",
    "        all_results[f\"{mode}_diag\"]={\n",
    "            \"ari_cc\":ari_cc,\"ari_ag\":ari_ag,\n",
    "            \"precision_at_k\":p_at_k,\"recall_at_k\":r_at_k\n",
    "        }\n",
    "\n",
    "        # free memory\n",
    "        del Xp,Xn,Xall,Xtr,Xte,trained,ensemble_fn,diag_pairs,diag_scores\n",
    "        gc.collect()\n",
    "\n",
    "    return all_results\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    warnings.filterwarnings(\"ignore\",category=UserWarning)\n",
    "    warnings.filterwarnings(\"ignore\",category=FutureWarning)\n",
    "    logging.basicConfig(level=logging.INFO,\n",
    "        format=\"%(asctime)s [%(levelname)s] %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    if torch:\n",
    "        logging.info(f\"PyTorch {torch.__version__}; CUDA={'yes' if torch.cuda.is_available() else 'no'}\")\n",
    "\n",
    "    # load data once\n",
    "    X, labels, heimild, birthyrs = load_training_data_with_details()\n",
    "\n",
    "    # prepare held-out record split (constant across runs)\n",
    "    full_labeled = np.where(labels!=-1)[0]\n",
    "    _, heldout = train_test_split(full_labeled, test_size=0.2, random_state=RNG_SEED)\n",
    "    heldout = heldout.tolist()\n",
    "\n",
    "    # run NUM_RUNS times\n",
    "    all_runs = []\n",
    "    for run_id in range(NUM_RUNS):\n",
    "        res = run_once(run_id, X, labels, heimild, birthyrs, heldout)\n",
    "        all_runs.append(res)\n",
    "\n",
    "    # average metrics across runs\n",
    "    avg = {}\n",
    "    for mode_stage in all_runs[0].keys():\n",
    "        # collect values per run\n",
    "        vals = {k:[] for k in all_runs[0][mode_stage]}\n",
    "        for r in all_runs:\n",
    "            for k,v in r[mode_stage].items():\n",
    "                vals[k].append(v)\n",
    "        avg[mode_stage] = {k: float(np.mean(vs)) for k,vs in vals.items()}\n",
    "\n",
    "    # — save per-mode & combined averaged metrics —\n",
    "    for mode in (\"within\", \"across\"):\n",
    "        mode_avg = {\n",
    "            \"test\":       avg[f\"{mode}_test\"],\n",
    "            \"diagnostics\": avg[f\"{mode}_diag\"]\n",
    "        }\n",
    "        mode_dir = BASE_REPORT / mode\n",
    "        mode_dir.mkdir(exist_ok=True, parents=True)\n",
    "        (mode_dir / \"averaged_metrics.json\").write_text(json.dumps(mode_avg, indent=2))\n",
    "\n",
    "        flat = {\"mode\": mode}\n",
    "        flat.update({f\"test_{k}\": v for k, v in mode_avg[\"test\"].items()})\n",
    "        flat.update({f\"diag_{k}\": v for k, v in mode_avg[\"diagnostics\"].items()})\n",
    "        pd.DataFrame([flat]).to_csv(mode_dir / \"averaged_metrics.csv\", index=False)\n",
    "\n",
    "    # combined average across both modes\n",
    "    combined = {}\n",
    "    for k in avg[\"within_test\"]:\n",
    "        combined[f\"test_{k}\"] = 0.5 * (avg[\"within_test\"][k] + avg[\"across_test\"][k])\n",
    "    for k in avg[\"within_diag\"]:\n",
    "        combined[f\"diag_{k}\"] = 0.5 * (avg[\"within_diag\"][k] + avg[\"across_diag\"][k])\n",
    "\n",
    "    comb_dir = BASE_REPORT / \"combined\"\n",
    "    comb_dir.mkdir(exist_ok=True, parents=True)\n",
    "    (comb_dir / \"averaged_metrics.json\").write_text(json.dumps(combined, indent=2))\n",
    "    pd.DataFrame([{\"mode\": \"combined\", **combined}]) \\\n",
    "        .to_csv(comb_dir / \"averaged_metrics.csv\", index=False)\n",
    "\n",
    "    logging.info(\"✔ All runs complete; per-mode and combined averaged metrics written to reports/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b1786b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 19:35:42 [INFO] ▶ Total Deployment Run\n",
      "2025-05-11 19:35:42 [INFO] ▶ Loading trained models and thresholds\n",
      "2025-05-11 19:35:42 [INFO] Loaded age disparity threshold: 76 years.\n",
      "2025-05-11 19:35:42 [INFO] Loaded XGBoost Booster for 'within'\n",
      "2025-05-11 19:35:42 [INFO] Loaded model 'lgb' for 'within'\n",
      "2025-05-11 19:35:42 [INFO] Loaded model 'cat' for 'within'\n",
      "2025-05-11 19:35:43 [INFO] Loaded model 'rf' for 'within'\n",
      "2025-05-11 19:35:43 [INFO] Loaded XGBoost Booster for 'across'\n",
      "2025-05-11 19:35:43 [INFO] Loaded model 'lgb' for 'across'\n",
      "2025-05-11 19:35:43 [INFO] Loaded model 'cat' for 'across'\n",
      "2025-05-11 19:35:43 [INFO] Loaded decision threshold for 'within' model: 0.540\n",
      "2025-05-11 19:35:43 [WARNING] Metrics file for 'across' (reports_gpu/across/metrics.json) not found. Using default decision threshold 0.5.\n",
      "2025-05-11 19:35:43 [INFO] ⏱ Loading trained models and thresholds: 0.9s\n",
      "2025-05-11 19:35:43 [INFO] ▶ Loading and preparing all deployment data\n",
      "2025-05-11 19:35:43 [INFO] Loading and preparing deployment data...\n",
      "2025-05-11 19:35:43 [WARNING] artifacts_deploy_subset/iceid_ml_ready.npz not found; falling back to artifacts/iceid_ml_ready.npz\n",
      "2025-05-11 19:35:43 [WARNING] artifacts_deploy_subset/row_labels.csv not found; falling back to artifacts/row_labels.csv\n",
      "2025-05-11 19:35:43 [INFO] Loaded X_deploy: (984028, 50799). Deployment set has 984028 unique original IDs.\n",
      "2025-05-11 19:35:55 [INFO] Successfully prepared 984028 aligned records with raw attributes and blocking keys.\n",
      "2025-05-11 19:35:55 [INFO] ⏱ Loading and preparing all deployment data: 12.0s\n",
      "2025-05-11 19:35:55 [INFO] ▶ Generating candidate pairs via blocking logic\n",
      "2025-05-11 19:35:55 [INFO] Generating candidate pairs for deployment using custom string key blocking...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d4ddca511a94c049a4c6e88777cebff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Blocking by custom_key:   0%|          | 0/47046 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 21:51:00 [INFO] ⏱ Generating candidate pairs via blocking logic: 8105.7s\n",
      "2025-05-11 21:51:00 [INFO] ⏱ Total Deployment Run: 8118.5s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 503\u001b[39m\n\u001b[32m    501\u001b[39m \u001b[38;5;66;03m# 3. Generate candidate pairs using blocking rules\u001b[39;00m\n\u001b[32m    502\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Timer(\u001b[33m\"\u001b[39m\u001b[33mGenerating candidate pairs via blocking logic\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m     candidate_pairs_within_mode, candidate_pairs_across_mode = \u001b[43mgenerate_candidate_pairs_for_deployment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpeople_attributes_for_deploy_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mage_disparity_param\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[38;5;66;03m# 4. Process \"within-census\" candidate pairs\u001b[39;00m\n\u001b[32m    508\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Timer(\u001b[33m\"\u001b[39m\u001b[33mProcessing \u001b[39m\u001b[33m'\u001b[39m\u001b[33mwithin-census\u001b[39m\u001b[33m'\u001b[39m\u001b[33m candidate pairs\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 300\u001b[39m, in \u001b[36mgenerate_candidate_pairs_for_deployment\u001b[39m\u001b[34m(df_with_blocking_attrs, age_disp_thresh)\u001b[39m\n\u001b[32m    297\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(i + \u001b[32m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(indices_in_block)):\n\u001b[32m    298\u001b[39m     idx1, idx2 = indices_in_block[i], indices_in_block[j]\n\u001b[32m--> \u001b[39m\u001b[32m300\u001b[39m     rec1_attrs = \u001b[43mdf_with_blocking_attrs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx1\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    301\u001b[39m     rec2_attrs = df_with_blocking_attrs.loc[idx2]\n\u001b[32m    303\u001b[39m     \u001b[38;5;66;03m# Filter 1: Gender (must match if both known and different from -1)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/ICE-ID-2.0-K4yMlCOc/lib/python3.11/site-packages/pandas/core/indexing.py:1191\u001b[39m, in \u001b[36m_LocationIndexer.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1189\u001b[39m maybe_callable = com.apply_if_callable(key, \u001b[38;5;28mself\u001b[39m.obj)\n\u001b[32m   1190\u001b[39m maybe_callable = \u001b[38;5;28mself\u001b[39m._check_deprecated_callable_usage(key, maybe_callable)\n\u001b[32m-> \u001b[39m\u001b[32m1191\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/ICE-ID-2.0-K4yMlCOc/lib/python3.11/site-packages/pandas/core/indexing.py:1431\u001b[39m, in \u001b[36m_LocIndexer._getitem_axis\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1429\u001b[39m \u001b[38;5;66;03m# fall thru to straight lookup\u001b[39;00m\n\u001b[32m   1430\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_key(key, axis)\n\u001b[32m-> \u001b[39m\u001b[32m1431\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_label\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/ICE-ID-2.0-K4yMlCOc/lib/python3.11/site-packages/pandas/core/indexing.py:1381\u001b[39m, in \u001b[36m_LocIndexer._get_label\u001b[39m\u001b[34m(self, label, axis)\u001b[39m\n\u001b[32m   1379\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_label\u001b[39m(\u001b[38;5;28mself\u001b[39m, label, axis: AxisInt):\n\u001b[32m   1380\u001b[39m     \u001b[38;5;66;03m# GH#5567 this will fail if the label is not present in the axis.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1381\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mxs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/ICE-ID-2.0-K4yMlCOc/lib/python3.11/site-packages/pandas/core/generic.py:4321\u001b[39m, in \u001b[36mNDFrame.xs\u001b[39m\u001b[34m(self, key, axis, level, drop_level)\u001b[39m\n\u001b[32m   4315\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ndim == \u001b[32m1\u001b[39m:\n\u001b[32m   4316\u001b[39m     \u001b[38;5;66;03m# if we encounter an array-like and we only have 1 dim\u001b[39;00m\n\u001b[32m   4317\u001b[39m     \u001b[38;5;66;03m# that means that their are list/ndarrays inside the Series!\u001b[39;00m\n\u001b[32m   4318\u001b[39m     \u001b[38;5;66;03m# so just return them (GH 6394)\u001b[39;00m\n\u001b[32m   4319\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[loc]\n\u001b[32m-> \u001b[39m\u001b[32m4321\u001b[39m new_mgr = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfast_xs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4323\u001b[39m result = \u001b[38;5;28mself\u001b[39m._constructor_sliced_from_mgr(new_mgr, axes=new_mgr.axes)\n\u001b[32m   4324\u001b[39m result._name = \u001b[38;5;28mself\u001b[39m.index[loc]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/ICE-ID-2.0-K4yMlCOc/lib/python3.11/site-packages/pandas/core/internals/managers.py:984\u001b[39m, in \u001b[36mBlockManager.fast_xs\u001b[39m\u001b[34m(self, loc)\u001b[39m\n\u001b[32m    976\u001b[39m     block = new_block(\n\u001b[32m    977\u001b[39m         result,\n\u001b[32m    978\u001b[39m         placement=bp,\n\u001b[32m    979\u001b[39m         ndim=\u001b[32m1\u001b[39m,\n\u001b[32m    980\u001b[39m         refs=\u001b[38;5;28mself\u001b[39m.blocks[\u001b[32m0\u001b[39m].refs,\n\u001b[32m    981\u001b[39m     )\n\u001b[32m    982\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SingleBlockManager(block, \u001b[38;5;28mself\u001b[39m.axes[\u001b[32m0\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m984\u001b[39m dtype = \u001b[43minterleaved_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mblk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mblk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    986\u001b[39m n = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    988\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, ExtensionDtype):\n\u001b[32m    989\u001b[39m     \u001b[38;5;66;03m# TODO: use object dtype as workaround for non-performant\u001b[39;00m\n\u001b[32m    990\u001b[39m     \u001b[38;5;66;03m#  EA.__setitem__ methods. (primarily ArrowExtensionArray.__setitem__\u001b[39;00m\n\u001b[32m    991\u001b[39m     \u001b[38;5;66;03m#  when iteratively setting individual values)\u001b[39;00m\n\u001b[32m    992\u001b[39m     \u001b[38;5;66;03m#  https://github.com/pandas-dev/pandas/pull/54508#issuecomment-1675827918\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/ICE-ID-2.0-K4yMlCOc/lib/python3.11/site-packages/pandas/core/internals/base.py:394\u001b[39m, in \u001b[36minterleaved_dtype\u001b[39m\u001b[34m(dtypes)\u001b[39m\n\u001b[32m    391\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(dtypes):\n\u001b[32m    392\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m394\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind_common_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtypes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/ICE-ID-2.0-K4yMlCOc/lib/python3.11/site-packages/pandas/core/dtypes/cast.py:1484\u001b[39m, in \u001b[36mfind_common_type\u001b[39m\u001b[34m(types)\u001b[39m\n\u001b[32m   1481\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m t.kind \u001b[38;5;129;01min\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33miufc\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1482\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m np.dtype(\u001b[33m\"\u001b[39m\u001b[33mobject\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1484\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp_find_common_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# deploy_ensemble_blocking_final.py\n",
    "# -----------------------------------------------------------------------------\n",
    "# - Loads trained ensemble models and age/task thresholds.\n",
    "# - Loads ML features for deployment records (X_deploy.npz).\n",
    "# - Loads raw attributes for deployment records by:\n",
    "#   - Reading row_labels_deploy.csv to identify deployment record IDs.\n",
    "#   - Loading and filtering full people.csv and manntol_einstaklingar_new.csv.\n",
    "#   - Joining to get necessary attributes (names, birthyear, sex, parish_id).\n",
    "# - Derives primary name part and given name for blocking.\n",
    "# - Generates a custom string prefix key for blocking.\n",
    "# - Applies filters: gender, age disparity, cross-census parish mismatch.\n",
    "# - Predicts on candidate pairs in batches & outputs clusters.\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "import itertools\n",
    "import json\n",
    "import joblib\n",
    "import logging\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Attempt to import for model loading if models were saved as specific types\n",
    "# (joblib usually handles this fine if the libraries are in the environment)\n",
    "# from xgboost import XGBClassifier \n",
    "# from lightgbm import LGBMClassifier\n",
    "# from catboost import CatBoostClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# ───────────── paths / constants ─────────────\n",
    "# --- Input Paths ---\n",
    "# Root directory for original data files (people.csv, manntol_einstaklingar_new.csv)\n",
    "ORIGINAL_DATA_DIR = Path(\"raw_data\") \n",
    "\n",
    "# Directory where preprocessing script outputs for the DEPLOYMENT SUBSET are stored\n",
    "# This dir should contain: X_deploy.npz (as iceid_ml_ready.npz) and row_labels_deploy.csv (as row_labels.csv)\n",
    "# If your preprocessing script always outputs to 'artifacts', then this might be 'artifacts'\n",
    "# Or, you might copy the relevant outputs for the deployment set to a dedicated directory.\n",
    "# For clarity, let's assume a dedicated dir for these inputs, or adjust if using 'artifacts' directly.\n",
    "DEPLOY_PREPROCESSED_INPUT_DIR = Path(\"artifacts_deploy_subset\") # Contains outputs of preprocessing on 800k\n",
    "DEPLOY_PREPROCESSED_INPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "# Expected files in DEPLOY_PREPROCESSED_INPUT_DIR:\n",
    "#   - iceid_ml_ready.npz (features for the deployment records)\n",
    "#   - row_labels.csv (mapping rows of the above NPZ to original 'id's)\n",
    "\n",
    "# Paths to trained models and training reports (for thresholds)\n",
    "TRAINED_MODEL_BASE_DIR = Path(\"models_ensemble_gpu\")\n",
    "TRAINING_REPORTS_BASE_DIR = Path(\"reports_gpu\")\n",
    "\n",
    "# --- Output Paths ---\n",
    "DEPLOY_OUTPUT_DIR = Path(\"deploy_output_final\"); DEPLOY_OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# --- Blocking / Prediction Constants ---\n",
    "CANDIDATE_PAIR_BATCH_SIZE = 50000 \n",
    "# For custom_blocking_key generation:\n",
    "# Using patronym/surname and first given name.\n",
    "# Adjust prefix lengths based on name characteristics and desired block granularity.\n",
    "PRIMARY_NAME_PART_PREFIX_LEN = 5 \n",
    "GIVEN_NAME_PREFIX_LEN = 3   \n",
    "\n",
    "# ───────────── helpers ─────────────\n",
    "class Timer:\n",
    "    def __init__(self, msg):\n",
    "        self.msg, self.t0 = msg, time.perf_counter()\n",
    "    def __enter__(self):\n",
    "        logging.info(f\"▶ {self.msg}\")\n",
    "        return self\n",
    "    def __exit__(self, *_):\n",
    "        logging.info(f\"⏱ {self.msg}: {time.perf_counter() - self.t0:.1f}s\")\n",
    "\n",
    "def generate_custom_blocking_key_from_parts(primary_name_val, first_given_name_val):\n",
    "    pn_str = str(primary_name_val).strip().upper()\n",
    "    gn_str = str(first_given_name_val).strip().upper()\n",
    "\n",
    "    pn_prefix = pn_str[:PRIMARY_NAME_PART_PREFIX_LEN]\n",
    "    gn_prefix = gn_str[:GIVEN_NAME_PREFIX_LEN]\n",
    "    \n",
    "    if not pn_prefix: pn_prefix = \"XPN\" \n",
    "    if not gn_prefix: gn_prefix = \"XGN\" \n",
    "        \n",
    "    return f\"{pn_prefix}_{gn_prefix}\"\n",
    "\n",
    "# ───────────────────────────────────────\n",
    "#  Load Models and Thresholds from Training\n",
    "# ───────────────────────────────────────\n",
    "def load_ensemble_models_for_deployment(mode):\n",
    "    model_dir = TRAINED_MODEL_BASE_DIR / mode\n",
    "    models_loaded = {}\n",
    "    # First, look for XGB Booster\n",
    "    xgb_path = model_dir / \"xgb.booster\"\n",
    "    if xgb_path.exists():\n",
    "        booster = xgb.Booster()\n",
    "        booster.load_model(str(xgb_path))\n",
    "        models_loaded[\"xgb\"] = booster\n",
    "        logging.info(f\"Loaded XGBoost Booster for '{mode}'\")\n",
    "    else:\n",
    "        # fallback to old .pkl if you still have it\n",
    "        pkl = model_dir / \"xgb.pkl\"\n",
    "        if pkl.exists():\n",
    "            models_loaded[\"xgb\"] = joblib.load(pkl)\n",
    "            logging.info(f\"Loaded XGBClassifier(pkl) for '{mode}'\")\n",
    "\n",
    "    # Now the other three\n",
    "    for name in (\"lgb\",\"cat\",\"rf\"):\n",
    "        p = model_dir / f\"{name}.pkl\"\n",
    "        if p.exists():\n",
    "            models_loaded[name] = joblib.load(p)\n",
    "            logging.info(f\"Loaded model '{name}' for '{mode}'\")\n",
    "\n",
    "    if not models_loaded:\n",
    "        raise FileNotFoundError(f\"No models found in {model_dir}\")\n",
    "\n",
    "    # unify Booster vs sklearn API\n",
    "    def _prob(m, X):\n",
    "        if isinstance(m, xgb.Booster):\n",
    "            return m.predict(xgb.DMatrix(X))\n",
    "        else:\n",
    "            return m.predict_proba(X)[:,1]\n",
    "\n",
    "    return lambda M: np.column_stack([_prob(m, M) for m in models_loaded.values()]).mean(axis=1)\n",
    "\n",
    "def load_age_threshold_from_training():\n",
    "    # Path where training script saved the age disparity threshold\n",
    "    threshold_file = TRAINING_REPORTS_BASE_DIR / \"_global_training_outputs\" / \"age_disparity_threshold.json\"\n",
    "    if not threshold_file.exists():\n",
    "        logging.error(f\"Age disparity threshold file not found: {threshold_file}. Using fallback default of 10 years.\")\n",
    "        return 10 \n",
    "    with open(threshold_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    age_thresh = data.get(\"age_disparity_threshold_years\", 10) # Default if key missing\n",
    "    logging.info(f\"Loaded age disparity threshold: {age_thresh} years.\")\n",
    "    return age_thresh\n",
    "\n",
    "def load_decision_threshold_for_task(mode):\n",
    "    # Path where training script saved metrics, including the decision threshold\n",
    "    metrics_file = TRAINING_REPORTS_BASE_DIR / mode / \"metrics.json\"\n",
    "    try:\n",
    "        with open(metrics_file, 'r') as f:\n",
    "            decision_thresh = json.load(f).get('threshold', 0.5) # Default if key missing\n",
    "        logging.info(f\"Loaded decision threshold for '{mode}' model: {decision_thresh:.3f}\")\n",
    "        return decision_thresh\n",
    "    except FileNotFoundError:\n",
    "        logging.warning(f\"Metrics file for '{mode}' ({metrics_file}) not found. Using default decision threshold 0.5.\")\n",
    "        return 0.5\n",
    "\n",
    "# ───────────────────────────────────────\n",
    "#  Load and Prepare Deployment Data (ML Features + Raw Attributes for Blocking)\n",
    "# ───────────────────────────────────────\n",
    "def load_and_prepare_deployment_data():\n",
    "    logging.info(\"Loading and preparing deployment data...\")\n",
    "    \n",
    "    # 1. Load ML features (X_deploy) and the mapping from its rows to original IDs\n",
    "    X_deploy_path = DEPLOY_PREPROCESSED_INPUT_DIR / \"iceid_ml_ready.npz\"\n",
    "    if not X_deploy_path.exists():\n",
    "        # fallback to the main artifacts directory\n",
    "        fallback = ART_DIR / \"iceid_ml_ready.npz\"\n",
    "        if fallback.exists():\n",
    "            logging.warning(f\"{X_deploy_path} not found; falling back to {fallback}\")\n",
    "            X_deploy_path = fallback\n",
    "        else:\n",
    "            raise FileNotFoundError(\n",
    "                f\"Deployment feature file not found in either {DEPLOY_PREPROCESSED_INPUT_DIR} or {ART_DIR}\"\n",
    "            )\n",
    "    X_deploy_features = sparse.load_npz(X_deploy_path)\n",
    "    \n",
    "    row_labels_for_deploy_path = DEPLOY_PREPROCESSED_INPUT_DIR / \"row_labels.csv\"\n",
    "    if not row_labels_for_deploy_path.exists():\n",
    "        # fallback to the main artifacts directory\n",
    "        fallback = ART_DIR / \"row_labels.csv\"\n",
    "        if fallback.exists():\n",
    "            logging.warning(f\"{row_labels_for_deploy_path} not found; falling back to {fallback}\")\n",
    "            row_labels_for_deploy_path = fallback\n",
    "        else:\n",
    "            raise FileNotFoundError(\n",
    "                f\"Row labels file not found in either {DEPLOY_PREPROCESSED_INPUT_DIR} or {ART_DIR}\"\n",
    "            )\n",
    "    # Columns: 'row_id' (original ID), 'person' (will be -1 or empty for unlabeled)\n",
    "    deploy_row_to_original_id_df = pd.read_csv(row_labels_for_deploy_path, dtype={'row_id': str})\n",
    "    deploy_original_ids = deploy_row_to_original_id_df['row_id'].unique() # Get unique original IDs for the deployment set\n",
    "    \n",
    "    logging.info(f\"Loaded X_deploy: {X_deploy_features.shape}. Deployment set has {len(deploy_original_ids)} unique original IDs.\")\n",
    "\n",
    "    # 2. Load raw attributes from original full data files, then filter for deployment IDs\n",
    "    # Load people.csv (full)\n",
    "    people_full_path = ORIGINAL_DATA_DIR / \"people.csv\"\n",
    "    if not people_full_path.exists(): raise FileNotFoundError(f\"{people_full_path} not found.\")\n",
    "    people_full_df = pd.read_csv(\n",
    "        people_full_path,\n",
    "        usecols=[\"id\", \"heimild\", \"first_name\", \"patronym\", \"surname\", \"birthyear\", \"sex\"],\n",
    "        low_memory=False,\n",
    "        dtype={\"id\": str, \"first_name\": str, \"patronym\": str, \"surname\": str, \"sex\": str}\n",
    "    )\n",
    "    # Filter for deployment IDs\n",
    "    people_deploy_subset_df = people_full_df[people_full_df['id'].isin(deploy_original_ids)].set_index('id')\n",
    "\n",
    "    # Load manntol_einstaklingar_new.csv (full) for parish_id ('bi_sokn')\n",
    "    manntol_full_path = ORIGINAL_DATA_DIR / \"manntol_einstaklingar_new.csv\" # Make sure this name is correct\n",
    "    if not manntol_full_path.exists(): raise FileNotFoundError(f\"{manntol_full_path} not found.\")\n",
    "    manntol_full_df = pd.read_csv(\n",
    "        manntol_full_path,\n",
    "        usecols=[\"id\", \"bi_sokn\"], # 'bi_sokn' is used as parish_id in preprocessing\n",
    "        dtype={\"id\": str, \"bi_sokn\": str}\n",
    "    )\n",
    "    # Filter for deployment IDs and select parish_id\n",
    "    manntol_deploy_subset_df = manntol_full_df[manntol_full_df['id'].isin(deploy_original_ids)].set_index('id')\n",
    "    \n",
    "    # Join to get parish_id with other people attributes\n",
    "    raw_attrs_deploy_df = people_deploy_subset_df.join(manntol_deploy_subset_df['bi_sokn'].rename(\"parish_id_raw\"), how=\"left\")\n",
    "    raw_attrs_deploy_df = raw_attrs_deploy_df.reset_index() # 'id' becomes a column again\n",
    "\n",
    "    # 3. Align these raw attributes with X_deploy_features (order by X_deploy's row index)\n",
    "    # deploy_row_to_original_id_df has 'row_id' (original id) and implicitly its index is X_deploy's row index\n",
    "    # We need to ensure people_attrs_aligned_df is indexed 0..N-1 corresponding to X_deploy rows\n",
    "    \n",
    "    # Map: X_row_index -> original_id -> raw_attributes\n",
    "    # Start with deploy_row_to_original_id_df which has 'row_id' (original ID)\n",
    "    # Its index (0..X_deploy.shape[0]-1) is what we want for the final aligned DFl.\n",
    "    final_aligned_attrs_df = deploy_row_to_original_id_df.copy()\n",
    "    final_aligned_attrs_df = final_aligned_attrs_df.rename(columns={'row_id': 'id'}) # 'id' now holds original ID\n",
    "    final_aligned_attrs_df = final_aligned_attrs_df.set_index('id').join(\n",
    "        raw_attrs_deploy_df.set_index('id'), # raw_attrs_deploy_df has all needed raw cols\n",
    "        how=\"left\"\n",
    "    )\n",
    "    final_aligned_attrs_df = final_aligned_attrs_df.reset_index() # 'id' is original_id\n",
    "    # Ensure it's sorted/reindexed to match X_deploy_features's implicit 0..N-1 row indexing\n",
    "    # If deploy_row_to_original_id_df was already sorted by its original index (0..N-1), this should be fine.\n",
    "    # For safety, explicitly create a row_index column if not present and sort by it.\n",
    "    if 'row_index' not in final_aligned_attrs_df.columns: # Should not happen if from row_labels.csv format\n",
    "         final_aligned_attrs_df['row_index'] = range(len(final_aligned_attrs_df)) # Assume it was implicitly ordered\n",
    "    final_aligned_attrs_df = final_aligned_attrs_df.set_index('row_index').sort_index()\n",
    "\n",
    "\n",
    "    if len(final_aligned_attrs_df) != X_deploy_features.shape[0]:\n",
    "        raise ValueError(\"CRITICAL: Mismatch after aligning raw attributes with X_deploy features. \"\n",
    "                         f\"Aligned: {len(final_aligned_attrs_df)}, X_deploy: {X_deploy_features.shape[0]}\")\n",
    "\n",
    "    # 4. Derive additional fields needed for blocking from raw attributes\n",
    "    # Sex_male_raw (0 for female, 1 for male, -1 for unknown)\n",
    "    final_aligned_attrs_df['sex_male_raw'] = final_aligned_attrs_df['sex'].apply(\n",
    "        lambda x: 1 if isinstance(x, str) and x.lower() == \"karl\" else (0 if isinstance(x, str) and x.lower() == \"kona\" else -1)\n",
    "    ).astype(int)\n",
    "\n",
    "    # Primary name part for blocking (Patronym if available, else Surname)\n",
    "    final_aligned_attrs_df['patronym'] = final_aligned_attrs_df['patronym'].fillna('')\n",
    "    final_aligned_attrs_df['surname'] = final_aligned_attrs_df['surname'].fillna('')\n",
    "    final_aligned_attrs_df['primary_name_part_for_blocking'] = final_aligned_attrs_df.apply(\n",
    "        lambda row: row['patronym'] if row['patronym'] else row['surname'], axis=1\n",
    "    )\n",
    "    # Given name for blocking\n",
    "    final_aligned_attrs_df['given_name_for_blocking'] = final_aligned_attrs_df['first_name'].fillna('')\n",
    "\n",
    "    # Custom blocking key\n",
    "    final_aligned_attrs_df['custom_blocking_key'] = final_aligned_attrs_df.apply(\n",
    "        lambda row: generate_custom_blocking_key_from_parts(\n",
    "            row['primary_name_part_for_blocking'], row['given_name_for_blocking']\n",
    "        ), axis=1\n",
    "    )\n",
    "    \n",
    "    # Convert relevant columns to expected types\n",
    "    final_aligned_attrs_df['heimild'] = pd.to_numeric(final_aligned_attrs_df['heimild'], errors='coerce').fillna(-1).astype(int)\n",
    "    final_aligned_attrs_df['birthyear'] = pd.to_numeric(final_aligned_attrs_df['birthyear'], errors='coerce').fillna(0).astype(int)\n",
    "    final_aligned_attrs_df['parish_id_raw'] = final_aligned_attrs_df['parish_id_raw'].astype(str).fillna('')\n",
    "\n",
    "\n",
    "    logging.info(f\"Successfully prepared {len(final_aligned_attrs_df)} aligned records with raw attributes and blocking keys.\")\n",
    "    # Important columns in final_aligned_attrs_df (indexed 0..N-1 like X_deploy):\n",
    "    # 'id' (original ID), 'heimild', 'birthyear', 'sex_male_raw', 'parish_id_raw', 'custom_blocking_key'\n",
    "    return X_deploy_features, final_aligned_attrs_df\n",
    "\n",
    "# ───────────────────────────────────────\n",
    "#  Blocking and Candidate Pair Generation (using derived attributes)\n",
    "# ───────────────────────────────────────\n",
    "def generate_candidate_pairs_for_deployment(df_with_blocking_attrs, age_disp_thresh):\n",
    "    # df_with_blocking_attrs is indexed 0..N-1 (like X_deploy)\n",
    "    # It contains: 'custom_blocking_key', 'sex_male_raw', 'birthyear', 'heimild', 'parish_id_raw'\n",
    "    logging.info(\"Generating candidate pairs for deployment using custom string key blocking...\")\n",
    "    cand_pairs_within = []\n",
    "    cand_pairs_across = []\n",
    "\n",
    "    # Group by the generated custom_blocking_key\n",
    "    grouped_by_key = df_with_blocking_attrs.groupby('custom_blocking_key')\n",
    "    \n",
    "    for block_key, group_from_key in tqdm(grouped_by_key, desc=\"Blocking by custom_key\"):\n",
    "        if len(group_from_key) < 2: continue # Need at least two records in a block\n",
    "\n",
    "        # Get record indices (0..N-1 for X_deploy) within this block\n",
    "        indices_in_block = group_from_key.index.tolist()\n",
    "\n",
    "        for i in range(len(indices_in_block)):\n",
    "            for j in range(i + 1, len(indices_in_block)):\n",
    "                idx1, idx2 = indices_in_block[i], indices_in_block[j]\n",
    "                \n",
    "                rec1_attrs = df_with_blocking_attrs.loc[idx1]\n",
    "                rec2_attrs = df_with_blocking_attrs.loc[idx2]\n",
    "\n",
    "                # Filter 1: Gender (must match if both known and different from -1)\n",
    "                if rec1_attrs['sex_male_raw'] != -1 and rec2_attrs['sex_male_raw'] != -1 and \\\n",
    "                   rec1_attrs['sex_male_raw'] != rec2_attrs['sex_male_raw']:\n",
    "                    continue\n",
    "\n",
    "                # Filter 2: Age Disparity (if both birthyears are valid)\n",
    "                # Assuming birthyear 0 means unknown/invalid after preprocessing\n",
    "                if rec1_attrs['birthyear'] > 0 and rec2_attrs['birthyear'] > 0:\n",
    "                    if abs(rec1_attrs['birthyear'] - rec2_attrs['birthyear']) > age_disp_thresh:\n",
    "                        continue\n",
    "                \n",
    "                # Determine if pair is for \"within\" or \"across\" model based on 'heimild'\n",
    "                is_same_heimild = rec1_attrs['heimild'] == rec2_attrs['heimild']\n",
    "                \n",
    "                # Ensure heimilds are valid if comparing (not -1)\n",
    "                heimild1_valid = rec1_attrs['heimild'] != -1\n",
    "                heimild2_valid = rec2_attrs['heimild'] != -1\n",
    "\n",
    "                if is_same_heimild and heimild1_valid: # Both records must have the same, valid heimild\n",
    "                    cand_pairs_within.append(tuple(sorted((idx1, idx2))))\n",
    "                elif not is_same_heimild and heimild1_valid and heimild2_valid: # Different, valid heimilds\n",
    "                    # Filter 3: Cross-Census Parish Mismatch (if parishes are known and different, not a match)\n",
    "                    parish1 = str(rec1_attrs['parish_id_raw']).strip()\n",
    "                    parish2 = str(rec2_attrs['parish_id_raw']).strip()\n",
    "                    if parish1 and parish2 and parish1 != parish2 : # Both non-empty and different\n",
    "                        continue \n",
    "                    cand_pairs_across.append(tuple(sorted((idx1, idx2))))\n",
    "\n",
    "    # Remove duplicates\n",
    "    final_cand_pairs_within = sorted(list(set(cand_pairs_within)))\n",
    "    final_cand_pairs_across = sorted(list(set(cand_pairs_across)))\n",
    "\n",
    "    logging.info(f\"Generated {len(final_cand_pairs_within)} 'within-census' candidate pairs after filtering.\")\n",
    "    logging.info(f\"Generated {len(final_cand_pairs_across)} 'across-census' candidate pairs after filtering.\")\n",
    "    return final_cand_pairs_within, final_cand_pairs_across\n",
    "\n",
    "# ───────────────────────────────────────\n",
    "#  Predict on Candidates & Cluster\n",
    "# ───────────────────────────────────────\n",
    "def predict_and_cluster_deployment(\n",
    "    X_features_deploy, \n",
    "    aligned_raw_attributes_df, # Indexed 0..N-1, contains 'id' (original_id)\n",
    "    list_of_candidate_pairs, \n",
    "    ensemble_predict_fn, \n",
    "    decision_thresh, \n",
    "    output_file_tag\n",
    "    ):\n",
    "    \n",
    "    logging.info(f\"Predicting on {len(list_of_candidate_pairs)} candidate pairs for '{output_file_tag}'...\")\n",
    "    \n",
    "    # For mapping X_features_deploy row indices back to original record IDs\n",
    "    # aligned_raw_attributes_df['id'] contains the original string IDs\n",
    "    original_id_lookup = aligned_raw_attributes_df['id'] \n",
    "\n",
    "    if not list_of_candidate_pairs:\n",
    "        logging.info(f\"No candidate pairs for '{output_file_tag}'. Creating empty output files.\")\n",
    "        empty_links_df = pd.DataFrame(columns=['original_id_1', 'original_id_2', 'probability', 'is_match'])\n",
    "        empty_links_df.to_csv(DEPLOY_OUTPUT_DIR / f\"{output_file_tag}_predicted_links.csv\", index=False)\n",
    "        \n",
    "        empty_clusters_df_data = [{\"original_id\": original_id_lookup.loc[i], \"cluster_id\": i} for i in range(X_features_deploy.shape[0])]\n",
    "        empty_clusters_df = pd.DataFrame(empty_clusters_df_data)\n",
    "        empty_clusters_df.to_csv(DEPLOY_OUTPUT_DIR / f\"{output_file_tag}_clusters.csv\", index=False)\n",
    "        return\n",
    "\n",
    "    all_predicted_probs = []\n",
    "    num_batches = (len(list_of_candidate_pairs) + CANDIDATE_PAIR_BATCH_SIZE - 1) // CANDIDATE_PAIR_BATCH_SIZE\n",
    "    \n",
    "    for i in tqdm(range(0, len(list_of_candidate_pairs), CANDIDATE_PAIR_BATCH_SIZE), \n",
    "                  total=num_batches, desc=f\"Predicting '{output_file_tag}' batches\"):\n",
    "        current_batch_pairs = list_of_candidate_pairs[i : i + CANDIDATE_PAIR_BATCH_SIZE]\n",
    "        if not current_batch_pairs: continue\n",
    "\n",
    "        # Construct feature matrix for this batch of pairs\n",
    "        # pair_matrix is defined in the training script, copy or re-import if separate\n",
    "        X_batch_for_model = sparse.vstack([sparse.hstack([X_features_deploy[p_idx1], X_features_deploy[p_idx2]]) \n",
    "                                           for p_idx1, p_idx2 in current_batch_pairs])\n",
    "        \n",
    "        batch_probs = ensemble_predict_fn(X_batch_for_model)\n",
    "        all_predicted_probs.extend(batch_probs)\n",
    "    \n",
    "    all_predicted_probs = np.array(all_predicted_probs)\n",
    "    \n",
    "    # Store all candidate pairs with their probabilities and match prediction\n",
    "    output_links_data = []\n",
    "    for k, (idx1, idx2) in enumerate(list_of_candidate_pairs):\n",
    "        prob = all_predicted_probs[k]\n",
    "        is_match_prediction = prob >= decision_thresh\n",
    "        output_links_data.append({\n",
    "            \"original_id_1\": original_id_lookup.loc[idx1], \n",
    "            \"original_id_2\": original_id_lookup.loc[idx2],\n",
    "            \"probability\": prob,\n",
    "            \"is_match\": is_match_prediction\n",
    "        })\n",
    "    \n",
    "    predicted_links_output_df = pd.DataFrame(output_links_data)\n",
    "    predicted_links_output_df.to_csv(DEPLOY_OUTPUT_DIR / f\"{output_file_tag}_predicted_links.csv\", index=False)\n",
    "    logging.info(f\"Saved {len(predicted_links_output_df)} candidate pair predictions for '{output_file_tag}'.\")\n",
    "\n",
    "    # Clustering using Connected Components on pairs predicted as matches\n",
    "    logging.info(f\"Clustering predicted matches for '{output_file_tag}'...\")\n",
    "    # Get pairs that are actual matches based on threshold\n",
    "    pairs_confirmed_as_matches = []\n",
    "    for k, (idx1, idx2) in enumerate(list_of_candidate_pairs):\n",
    "        if all_predicted_probs[k] >= decision_thresh:\n",
    "            pairs_confirmed_as_matches.append((idx1, idx2)) # Using X_features_deploy row indices\n",
    "\n",
    "    if not pairs_confirmed_as_matches:\n",
    "        logging.info(f\"No pairs predicted as matches for '{output_file_tag}'. All records are singletons.\")\n",
    "        # Each record is its own cluster\n",
    "        singleton_clusters_data = [{\"original_id\": original_id_lookup.loc[i], \"cluster_id\": i} \n",
    "                                   for i in range(X_features_deploy.shape[0])]\n",
    "        singleton_clusters_df = pd.DataFrame(singleton_clusters_data)\n",
    "        singleton_clusters_df.to_csv(DEPLOY_OUTPUT_DIR / f\"{output_file_tag}_clusters.csv\", index=False)\n",
    "        return\n",
    "\n",
    "    # Build graph only with confirmed matches\n",
    "    nodes_in_match_graph = sorted(list(set(idx for pair in pairs_confirmed_as_matches for idx in pair)))\n",
    "    \n",
    "    local_node_map = {node_idx: i for i, node_idx in enumerate(nodes_in_match_graph)}\n",
    "    num_local_graph_nodes = len(nodes_in_match_graph)\n",
    "    cc_parent = list(range(num_local_graph_nodes))\n",
    "\n",
    "    def find_cc_set(i_local_idx):\n",
    "        if cc_parent[i_local_idx] == i_local_idx: return i_local_idx\n",
    "        cc_parent[i_local_idx] = find_cc_set(cc_parent[i_local_idx])\n",
    "        return cc_parent[i_local_idx]\n",
    "\n",
    "    def unite_cc_sets(i_local_idx, j_local_idx):\n",
    "        root_i, root_j = find_cc_set(i_local_idx), find_cc_set(j_local_idx)\n",
    "        if root_i != root_j: cc_parent[root_j] = root_i # Union by making one root parent of other\n",
    "\n",
    "    for idx1_row, idx2_row in pairs_confirmed_as_matches:\n",
    "        # These indices are already for X_features_deploy\n",
    "        if idx1_row in local_node_map and idx2_row in local_node_map: # Should always be true\n",
    "            unite_cc_sets(local_node_map[idx1_row], local_node_map[idx2_row])\n",
    "    \n",
    "    # Assign final cluster IDs\n",
    "    # All records from X_features_deploy get a cluster_id\n",
    "    # Records not in any confirmed match pair become their own singleton cluster.\n",
    "    \n",
    "    # Map X_features_deploy row_index to a final cluster_id value\n",
    "    final_record_to_cluster_map = {} \n",
    "    current_global_cluster_id = 0\n",
    "    # Map local CC root index to a global cluster_id\n",
    "    local_root_to_global_cluster_id = {} \n",
    "\n",
    "    # Assign cluster IDs for records involved in matches\n",
    "    for i_row_idx in nodes_in_match_graph: # Iterate over X_features_deploy row indices in graph\n",
    "        local_idx = local_node_map[i_row_idx]\n",
    "        root_for_node = find_cc_set(local_idx)\n",
    "        if root_for_node not in local_root_to_global_cluster_id:\n",
    "            local_root_to_global_cluster_id[root_for_node] = current_global_cluster_id\n",
    "            current_global_cluster_id += 1\n",
    "        final_record_to_cluster_map[i_row_idx] = local_root_to_global_cluster_id[root_for_node]\n",
    "\n",
    "    # Assign cluster IDs for singleton records (not in any match)\n",
    "    output_cluster_assignment_data = []\n",
    "    for i_row_idx in range(X_features_deploy.shape[0]):\n",
    "        original_id_val = original_id_lookup.loc[i_row_idx]\n",
    "        if i_row_idx in final_record_to_cluster_map:\n",
    "            assigned_cluster_id = final_record_to_cluster_map[i_row_idx]\n",
    "        else: # Singleton\n",
    "            assigned_cluster_id = current_global_cluster_id\n",
    "            current_global_cluster_id += 1\n",
    "        output_cluster_assignment_data.append({\"original_id\": original_id_val, \"cluster_id\": assigned_cluster_id})\n",
    "        \n",
    "    final_clusters_df = pd.DataFrame(output_cluster_assignment_data)\n",
    "    final_clusters_df.to_csv(DEPLOY_OUTPUT_DIR / f\"{output_file_tag}_clusters.csv\", index=False)\n",
    "    logging.info(f\"Saved {len(final_clusters_df)} records with cluster assignments for '{output_file_tag}'. \"\n",
    "                 f\"Found {final_clusters_df['cluster_id'].nunique()} unique clusters.\")\n",
    "\n",
    "# ───────────────────────────────────────\n",
    "#  Main Deployment Logic\n",
    "# ───────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\", datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    with Timer(\"Total Deployment Run\"):\n",
    "        # 1. Load trained models and necessary thresholds\n",
    "        with Timer(\"Loading trained models and thresholds\"):\n",
    "            age_disparity_param = load_age_threshold_from_training()\n",
    "            # Load ensemble prediction functions\n",
    "            ensemble_model_within = load_ensemble_models_for_deployment(\"within\")\n",
    "            ensemble_model_across = load_ensemble_models_for_deployment(\"across\")\n",
    "            # Load decision thresholds for each task\n",
    "            decision_threshold_within = load_decision_threshold_for_task(\"within\")\n",
    "            decision_threshold_across = load_decision_threshold_for_task(\"across\")\n",
    "\n",
    "        # 2. Load and prepare deployment data (ML features and raw attributes for blocking)\n",
    "        with Timer(\"Loading and preparing all deployment data\"):\n",
    "            # X_deploy is the ML feature matrix (sparse)\n",
    "            # people_attributes_for_deploy_df is indexed 0..N-1 like X_deploy,\n",
    "            # and contains 'id'(original_id), 'heimild', 'birthyear', 'sex_male_raw', \n",
    "            # 'parish_id_raw', 'custom_blocking_key'.\n",
    "            X_deploy, people_attributes_for_deploy_df = load_and_prepare_deployment_data()\n",
    "\n",
    "        # 3. Generate candidate pairs using blocking rules\n",
    "        with Timer(\"Generating candidate pairs via blocking logic\"):\n",
    "            candidate_pairs_within_mode, candidate_pairs_across_mode = generate_candidate_pairs_for_deployment(\n",
    "                people_attributes_for_deploy_df, age_disparity_param\n",
    "            )\n",
    "\n",
    "        # 4. Process \"within-census\" candidate pairs\n",
    "        with Timer(\"Processing 'within-census' candidate pairs\"):\n",
    "            predict_and_cluster_deployment(\n",
    "                X_deploy, people_attributes_for_deploy_df, \n",
    "                candidate_pairs_within_mode, \n",
    "                ensemble_model_within, decision_threshold_within, \n",
    "                \"deployment_within_census\" # output file tag\n",
    "            )\n",
    "\n",
    "        # 5. Process \"across-census\" candidate pairs\n",
    "        with Timer(\"Processing 'across-census' candidate pairs\"):\n",
    "            predict_and_cluster_deployment(\n",
    "                X_deploy, people_attributes_for_deploy_df, \n",
    "                candidate_pairs_across_mode, \n",
    "                ensemble_model_across, decision_threshold_across, \n",
    "                \"deployment_across_census\" # output file tag\n",
    "            )\n",
    "\n",
    "    logging.info(\"--- Deployment Script Finished ---\")\n",
    "    logging.info(f\"All deployment outputs are in directory: {DEPLOY_OUTPUT_DIR}\")\n",
    "    logging.info(\"Review *_predicted_links.csv for pair probabilities and match status.\")\n",
    "    logging.info(\"Review *_clusters.csv for final entity cluster assignments (using original_id).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9ceb47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ICE-ID-2.0-K4yMlCOc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
